[{"title":"表达欲这种东西，输入多了自然就回来了","date":"2019-06-14T05:08:00.000Z","path":"2019/06/14/back/","text":"历经将近一年的沉寂，我还是决定写些无人问津的东西，作为 PhD 期间的感悟与记录了。 其实我有纠结过像花姐一样，将内心激进的学术与政治观点宣之于众，然而或许自己锋芒不足，无论是知乎文章还是豆瓣日记，权衡一番觉得还是不太适合 post 这些过于私人的观点和评论。这全然无关政治，只是我不想因为评论区智障的表演而洗眼，也懒得与 AI 猜谜、与管理员斗气。Anyway，在这个没有审查的角落里，我尚且有畅所欲言的锋芒与勇气。 表达欲的下降，追本溯源，是因为输入的贫乏*。而这篇 prelude 的突兀出现，也并非我一时兴起，实在是周三的第一次 Seminar 学了太多干货，跟着师兄读了 Lieb-Robinson Bounds 的原始文章，另外复现了 1D Lie-Schultz-Mattis theorem 的证明，寻一处宣泄两天过去都难以平复的表达欲罢了。换句话说，有文想写，有话想说，说明最近的输入与成长是令人满意的。但愿以后的日子，能日进斗才，而文思泉涌不涸。","tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]},{"title":"Printer Setup on Archlinux (CUPS)","date":"2018-09-13T20:22:40.000Z","path":"2018/09/13/Printer Setup on Archlinux (CUPS)/","text":"CUPS is the standards-based, open source printing system developed by Apple Inc. for macOS® and other UNIX®-like operating systems. Installation and ConfigurationFirst install necessary packages for CUPS services1yaourt -S cups cups-pdf libcups Then install drivers related to your printer. For me, the printer at kitchen of physics department of BC is of type RICOH Aficio MP 6002. So I installed1yaourt -S openprinting-ppds-postscript-ricoh openprinting-ppds-pxlmono-ricoh All pdd files will be stored at /usr/share/ppd/cupsfilters, then1cp Ricoh-PDF_Printer-PDF.ppd /home/hxd for future use. open the browser and input http://localhost:631/admin, try to add one printer. If you are encountered with Forbidden, that means you are not in the group permitting to use the printer service.First start the cups service 1sudo systemctl start org.cups.cupsd and check if it is running 1sudo systemctl status -l org.cups.cupsd Then add your username to lp group1sudo gpasswd -a $(USER) lp Also, since in configuration file of cups (at /etc/cups/cups-files.conf), the system group is defined as root and sys, we should add in our username as well1sudo usermod -a -G sys $(USER) Then restart the cups service (Necessary!)1sudo systemctl restart org.cups.cupsd Now you are able to add and configure printers, which demands the pdd files we found before. Usage To print on PDF file, you should first set the default printer. To check printers that are idle, type1lpstat -p -d Then choose the printer by name and type in (Here the pinter name is Printer)1lpoptions -d Printer then cd into the path of file, type in1lp $(name of file)","tags":[{"name":"Archlinux","slug":"Archlinux","permalink":"http://yoursite.com/tags/Archlinux/"}]},{"title":"Landau-Ginzberg Effective Theory on Deep Neural Network","date":"2017-11-02T06:14:27.000Z","path":"2017/11/02/Landau-Ginzberg-Effective-Theory-on-Deep-Neural-Network/","text":"Deep neural network (DNN) works in an amazing effeciency in machine learing but cannot be well-explained by accurate mathematics. Reviewing the one-to-one correspondence between Restricted Boltzmann machines (RBM) and configuration space renomalization groups (RG), the well-developed techniques in studying quantum fields theory (QFT), we are motivated to apply the effective fields theory, particularly Landau-Ginzberg one, to explain the expressibility and mechnism of a general DNN. Similar structures of the effective Hamiltionian are discovered, and we propose a new model based on the systematic cluster expansion approach to improve the performance of DNN without adding extra layer. Supported by the high efficiency in counting low orders Feynman diagrams in computation of scattering amplitute by QFT, the upgrade of DNN we propose is believed to provide a higher efficiency than extending the depth of DNN if the coarse graining procedure has neared the fixed point of RG flows. Exact Representation of Physical ModelsRenormalization Group of Ising ModelThe Boltzman machine in ML is a stochastic binary whose composition consists of stochastic neurons1. Coincidentally, in physics we have a large fields of binary-valued spin models on lattice to explain the wide properties of magnetism. A typical and famous one is the Ising model . Ising model is a $N$ spin qubit system dominated by the exchange interaction (here we consider the general case)$$H(\\{s\\},\\bm{K})=-\\sum_iK_i s_i-\\sum_{ij}K_{ij}s_i s_j-\\cdots.$$By statistical theory of canonical ensemble, we have the partition function $Z\\equiv\\mathrm{Tr}\\,\\hat{\\rho}=\\mathrm{Tr}\\,e^{-\\beta \\hat{H}(\\{s\\})}$ and the free energy $F=-kT\\ln Z$. Physicist has developed a systematic and powerful technique called renormalization group to describe the distinct behaviors of fields theory on different energy levels2, which was then generalized to the change of lengh scale as well and perfectly handled the description of divergence beahavior near critical point of phase transitions3. The heart of the point is, when we alter the scale of an arbitrary system, the physical laws behind it still holds without any change of form. To put it more explicitly, we take the Ising model here as one example, as is shown in fig:1 **Coarse graining process of a $8 imes8$ lattice**. In each step of rescaling (illustrated by different colors of dashed squares), spins in each block is averaged and renormalized but holds the configuration of the square lattice. Though introducing new blocks of spins and average as well as normalize them, i.e., $\\Sigma_I=\\frac{1}{R}\\sum_{i\\in I}s_i$, where $R$ is a normalize factor to make $\\Sigma_I$ still takes value of either $-1$ or $+1$, the details of initial spins distribution are cancelled out and we are left with the inner structure of the system. An this process is called \\emph{coarse graining}.\\parDenote $\\{s\\}$ as the initial spin distribution, $\\{s’\\}$ the next coarse grained ones and $\\lambda$ the scaling factor, by the scaling hypothesis7, we still have$$H^{RG}_\\lambda(\\{s’\\},\\bm{K’})=-\\sum_iK’_i s’_i-\\sum_{ij}K’_{ij}s’_i s’_j-\\cdots,$$where $\\bm{K’}$ describes the new interaction between coarse grained spins, or hidden spins. In mathematics literature, mapping $f_\\lambda:\\bm{K}\\rightarrow\\bm{K’}$ forms a semigroup action since it is irreversible, and we call $\\{f\\}$ the RG group. If the GR-group flow reach the fixed point in the parameter space[^Francesco-CFT]. That is,$$\\bm{K_c}=\\bm{f}(K_c),$$then the difference of free energy of two coarse graining process will vanish$$\\Delta F=0\\implies\\mathrm{Tr}\\,e^{-\\beta T_\\lambda(\\{s\\},\\{s’\\})}=0,$$where $T_\\lambda(\\{s\\},\\{s’\\})\\equiv H(\\{s\\},\\bm{K})-H_\\lambda^{RG}(\\{s’\\},\\bm{K’})$. Next, we are to map RG to the Restriced Boltzman Machine(RBM). The stochasitic neurons of the RBM are partitioned into two groups, visible neurons and hidden neurons. Visible neurons provide an interface between the network and the ambient environment and during the training process they are all clumped onto specific states determined by the environment. The hidden neurons, however, always operate freely that they are used to explain the underlying constraints contained in the environmental input vectors4.Coupling between one layer to the next coarse grained layer is characterized by the operator $T_\\lambda(\\{s\\},\\{s’\\})$, in RBM, in fact, we also has a analogous object called joint energy between hidden neurons and visible neurons$$\\begin{equation}\\label{2.1} E(\\{v_i\\},\\{h_j\\})=\\sum_jb_jh_j+\\sum_{ij}v_iw_ijb_j+\\sum_ic_iv_j,\\end{equation}$$where $\\{v\\}$ denote the visible unit while $\\{h_i\\}$ denote the hidden unit. **Restricted Boltzman Machine**. The connections between the visible and hidden neurons are symmetric. Unlike general BM, such symmetric connections do not extend to the visible and hidden neurons. Similarly we can define the joint probability distribution$$\\begin{equation}\\label{2.2} p_\\lambda(\\{v\\},\\{h\\})=\\dfrac{1}{Z}e^{-E(\\{v\\},\\{h\\})}\\end{equation}$$and variational distribution for visible neurons as well$$\\begin{equation}\\label{2.3} p_\\lambda(\\{v\\})\\equiv\\sum_{\\{h\\}}p_\\lambda(\\{v\\},\\{h\\})=\\mathrm{Tr}_{h}\\,p_\\lambda(\\{v\\},\\{h\\}).\\end{equation}$$And the training tast is to minimize the Kullback-Leibler divergence between the visible distribution to the true one$$\\begin{equation}\\label{2.4} D_{KL}\\big(P(\\{c\\})\\mid\\mid p_\\lambda(\\{v\\})\\big)=\\sum_{\\{v\\}}P(\\{v\\})\\ln\\left(\\dfrac{P(\\{v\\})}{p_\\lambda(\\{v\\})}\\right).\\end{equation}$$Coupling between coarse grained states are encoded by the operator $T_\\lambda(\\{v\\},\\{h\\})$, while in RBM an analogous role is played by the joint energy $E(\\{v\\},\\{h\\})$. In fact, equation$$\\begin{equation}\\label{2.5} T(\\{v\\},\\{h\\})=-E(\\{v\\},\\{h\\})+H[\\{v\\}].\\end{equation}$$one-to-one maps the RG scheme to deep RBM, as is proven below.\\parBy definition of $T$, we have$$\\dfrac{1}{Z}e^{-H_\\lambda^{RG}[\\{h\\}]}=\\dfrac{1}{Z}\\mathrm{Tr}_v\\,e^{T_\\lambda(\\{v\\},\\{h\\})-H[\\{v\\}]}.$$Substituting the claimed Eq. $\\eqref{2.5}$, one immediately gets$$H_\\lambda^{RG}[\\{h\\}]=H_\\lambda^{\\text{RBM}}[\\{h\\}]$$and we are done. Since $\\lambda$ in RBM indicates the two pair of hidden and visible layers, by this proposition we can make a corollary that the layer of DNN plays the same roles of RG flows reaching the fixed point. And this also explains why the depth of the NN counts. Understanding of Neural Network with Points of Effective Fields TheoryIn an arbitrary task of prediction in machine learning, probability distribution $p(y|x)$ is the crux we are concerned about, where $x$ is the vector in sampling space and $y$ the model variables. Explicitly, in a typical classification problem of aninmals, we interpret $x$ as features of testing pictures, such as vectors taking values in $\\{\\text{fur}, \\text{whikser}, \\text{eye},\\cdots\\}$ and $y$ as animals $\\{\\text{cat}, \\text{dog}, \\text{human}, \\cdots\\}$ we want to classify. As a physical example, we may interpret $x$ as the input spin configuration and $y$ the set of physical phases such as high/low temperature phases5 and long-length entangled topological states6.Anyway, by Bayer’s theorem, we always have$$p(y|x)=\\dfrac{p(x|y)p(y)}{\\displaystyle\\sum_{y’}p(x|y’)p(y’)}.$$If we endow the two negative logarithms with physical meanings as following:$$\\begin{align}H_y(x)&amp;:=-\\ln p(x|y),\\label{3.1}\\\\\\mu_y&amp;:=-\\ln p(y),\\label{3.2}\\end{align}$$then Bayer’s theorem can be expressed as a more familiar form in statistical physics$$\\begin{equation}\\label{3.3} p(y|x)=\\dfrac{1}{Z(x)}e^{-(H_y(x)-\\mu_y)},\\end{equation}$$where partition function$$Z(x)\\equiv\\displaystyle\\sum_y e^{-(H_y(x)-\\mu_y)}.$$where $H_y$ is Hamiltonian and $\\mu_y$ the chemical potential of the physical system. The recasting of conditional probability by Hamiltonian and introducing of chemical potential is NOT futile as it seems. One the one hand, the task of finding an arbitrary function of our system was converted to find a quantity of rich physical meaning, and then we can understand the mechinism of neuron network in an thoroughly different perspective and may even discern some inner structure of NN according to the propeties of Hamiltonian that is widely used in theoretical physics but not revealed by experts in ML yet. Landau-Ginzberg TheoryLet us start with reviewing the Landau-Ginzberg fields in effective theory of statistical fields. Through introducing variables under mesoscopic scales under the assumption8 that $\\lambda\\gg\\dd x\\gg a$, the partition function of microscopic states can always be re-expressed by the coarse grained mesoscopic fields$$\\begin{equation}\\label{3.4} Z(T)=\\mathrm{Tr}\\,e^{-\\beta H_{\\text{micro}}}=\\int{\\mathcal D}\\psi\\,\\mathcal{W}[\\psi(\\bm{x})],\\end{equation}$$where the measure $\\mathcal{D}\\psi$ in Feynman path integral[^nakahara2003geometry] runs over all the possible configuration of fields in the function space and $\\mathcal{W}[\\psi]$ is the weights of each configuration. Although obtaining the precise form of $\\mathcal{W}[\\psi]$ is actually not easier than exactly solving the full problem with accurate partition function, is is possible to describe it in terms of only a few phenominological parameters but not a large number of microscopic canonical variables. If we define the effective Hamiltonian of the system by$$\\beta H[\\psi(\\bm{x})]:=-\\ln\\mathcal{W}[\\psi(\\bm{x})],$$then it can be expanded to a general form that$$\\begin{equation}\\label{3.5} \\beta H=\\int \\dd\\bm{x}~\\Phi[\\bm{x},\\psi(\\bm{x}),\\nabla\\psi(\\bm{x}),\\nabla^2\\psi(\\bm{x}),\\cdots].\\end{equation}$$It seems that what we have done is merely equivalent mathematical transformation, but since now all the variable in our Hamiltonion are mesoscopic ones, we can apply the symmetries of system or impose extra demands of our desired theory such as locality and casuality. These simple consideration will dramatically simplify the above complicated expression. \\parIn condensed matter physics, solid lattice are ususually regared as invariant under translation (temporarilly neglect all the defects), so does the explicit form of $\\Phi$. So$$\\Phi[\\bm{x},\\psi(\\bm{x}),\\nabla\\psi(\\bm{x}),\\cdots]=\\Phi[\\psi(\\bm{x}),\\nabla\\psi(\\bm{x}),\\cdots].$$Likewise, lattice are also invariant under rotations, dropping all the odd power of gredient of fields operators:$$\\begin{align} \\beta H&amp;=\\int\\dd\\bm{x}~\\Phi[\\psi(\\bm{x})^2,\\nabla^2\\psi(\\bm{x}),\\cdots]\\\\ &amp;=\\int\\dd\\bm{x}~K_1\\psi^2+K_2\\psi^4+\\cdots+M_1(\\nabla\\psi)^2+M_2(\\nabla\\psi)^4\\nonumber\\\\ &amp;\\qquad+\\cdots+N_1\\psi^2(\\nabla\\psi)^2+N_2\\psi^4(\\nabla\\psi)^4+\\cdots.\\nonumber\\end{align}$$Most importantly, in many cases involving merely short-range interactions (even long-range interaction such as Coulumb one becomes short-range due to the screening effect9), locality guarantees that only the lower order terms make remarkable contribution in Hamiltonian, so we are finally left with$$\\begin{equation}\\label{3.6} \\beta H=\\int\\dd\\bm{x}~\\bigg(K_1\\psi^2+K_2(\\nabla\\psi)^2\\bigg)+\\text{interactive perturbation},\\end{equation}$$where contribution from interaction can be carefully analysis by counting Feynman diagrams2 (Though regularization and renomarlization in computation is still too tough for humans to do). Effective Polynomial of Hamiltonian in MLAlthough the procedure constructing Hamiltonian showing above leave an amazing impression in simplification, one may first doubt the implicit hypothesis that such polynomial effective Hamiltonian in physics is complete in ML, i.e., any wanted function can be accurately evaluated by a sufficient large network. In fact, by the celebrated Stone-Weierstrass thoerem in real analysis going that polynomial functions can approximate continuous functions in arbitrary accuracy, DNN, denoted by$$\\begin{equation}\\label{3.7} f(\\bm{x})=\\bm{\\sigma}_n\\bm{A}\\cdots\\bm{\\sigma}_2\\bm{A}_2\\bm{\\sigma}_1\\bm{A}_1\\bm{x}\\end{equation}$$in our notation, where $\\bm{\\sigma}_i$ indicate the non-linear activation function such as sigmoid, Relu, or softmax functions, and $\\bm{A}_i$ indicates affine transformation that $\\bm{A}_i\\bm{x}\\equiv\\bm{W}_i\\bm{x}+\\bm{b}_i$, should also does, provided that the polynomial structure still holds under the action of the neuron network $f$. In fact, a simple neural network of the form $f=\\bm{\\sigma}_2\\bm{\\sigma}\\bm{A}_1$ with input layer, hidden layer and output layer size 2,4 and 1, exactly satisfies this condition12. Therefore, in ML we can also merely focus on the polynomial series (at a specific layer $\\lambda$)$$\\begin{align}\\label{3.8}H_{y}(\\bm{x};\\lambda)&amp;=h(\\lambda)+\\sum_ih_{\\lambda i}x_i+\\sum_{i&lt;j}h_{ij}(\\lambda )x_ix_j\\nonumber\\\\&amp;+\\sum_{i&lt;j&lt;k}h_{ijk}(\\lambda)x_ix_jx_k+\\cdots,\\end{align}$$where $x_i$ are the $i$th component of the vector $\\bm{x}$ at layer $\\lambda$ (not have to be the same size as the input layer), and correlation between layers $\\{h,h_i,h_{ij},\\cdots\\}$ are also labeled by the coarse grained steps $\\lambda$. The similar form of Hamiltonian in ML is a reminiscent of that in Landau-Ginzberg theory. As is seen before, translation and rotation symmetry of the physical system is certainly not powerful in simplifying Hamiltonian as locality does, which truncates at finite terms and leaves with effective ones, regarding the other terms as perturbation. But we can still find similar operation in ML immitating the translation behaviors-convolution of grayscale pixels, or CNN. And interesting and strange enough, there hitherto seems to exist no appropriate algorithm about extracting rotational symmetry in pattern recognition. What really counts in simplifying DNN, in physics perspective, is condition of locality. In physics we attrbute the quadratic form of fundament equations deirved from variational derivatives of action to the finite correlation of separate parts of the system, where quadratic Hamiltonian is enough to grasp the free thoery. For example, the Maxwell equation governing the electromagnetism, the Navier-Stokes equations governing fluid dynamics, and even the non-linear Einstein’s equation of gravitational fields are all quadratic. Another explanation of the emergence of low order effective Hamiltonian central limit theorem, which dominate large number of probability distribution in ML. The commonality in fundamental equations of theoretical physics listed here does not indicate the same behavier on DNN, since we’ve not found an appropriate way to define the ``measure’’ of neurons in the same layer, but this does enlighten us a new angle to understand the training results of neuron network. After training of a huge neuron network, one may find some correlation between neurons, namely, tensor coefficients in Hamiltonian $h_{ijk\\cdots}$, look extremely sparse that has little contribution on evalution of output, and hence can be dropped out from the tumid network with little influence on the new slender one. In this sense, we can safely concludes that locality corresponds to sparsity in ML.A typical and non-trivial example is deep RBM we talked before, in which the joint energy (Hamiltonian) between every two visible neurons and hidden neurons is defined to be binanry form $\\eqref{2.1}$. Though the structure of RBM looks disparate from the general DNN we write before in $\\eqref{3.7}$, if we treat the visible-hidden two layers pairs as in the same ``layer’’ (in the sense I introduced in $\\eqref{3.7}$), i.e., they are now sub-layers of one specific layer, deep RBM still embody the spirit of effective theory with the Hamiltonian$$H_\\lambda=\\int\\dd\\bm{x}~\\bigg[\\bigg(K_1(\\lambda)\\psi+M_1(\\lambda)\\nabla\\psi\\bigg)+N_1(\\lambda)\\psi\\nabla\\psi\\bigg]$$if we apply the correspondence $h_i\\leftrightarrow\\psi$ and $v_i\\leftrightarrow\\nabla\\psi$. One can see that this is the lowest expanding of Hamiltonian with interactions $N_1(\\lambda)\\psi\\nabla psi$. The reason why we drop the other two two-order free parts $K_2(\\lambda)\\psi^2$ and $M_2(\\lambda)(\\nabla\\psi)^2$ is that we are now considering the RBM, rather than general BM. Motivated by the common procedures of counting Feynman diagrams in quantum fields theory, one should naturally attempt to include higher order terms unless we reach the optimization order of asymptotic series11 : $$H=H^{(1)}_\\text{free}+H^{(2)}_{\\text{int.}}+H^{(3)}_{\\text{high order}}+\\cdots,$$ where $H^{(3)}$ includes self-coupling terms $K_3\\psi^3$, $M_3(\\nabla\\psi)^3$ and high order interactions $N_{3,1}\\psi^2\\nabla\\psi$, $N_{3,1}\\psi(\\nabla\\psi)^2$. But at present the planar diagram in ML to visualize the DNN never works and we have to carefully count the coupling between neurons at the same layer (in the new sense) in the new notation used in cluster expansion introduced by Mayer8. **The Third Order Couplings between Neurons**. Unlike the notation widely used in ML that lines between neurons denotes the affine transformation or any other mathematical operations, here lines in diagrams just represent the couplings without any mathematical meaning. Black circles represent the hidden sub-layers while white ones represent the visible sub-layers. Taking the step in consideration of high-order neuron networks, we are now reaching an entirely new world that experts in ML seems to ignore. Hopefully adding an extra interaction among neuron in a layer may improve much more than adding layers behind the NN in practical training, since by our discussion before, adding layers just helps reaching close to the fixed point under actions of RG, but the real fixed points will differ from that described by the low order effective Hamiltonian. Conclusion and PerspectivesHowever, even though locality, or sparcity widely exists in physics and ML, people still discover some important exceptions in recent year. In fractional quantum hall effect, a strong-correlated system in condensed matter physics, for example, repulsive interaction between electrons are not screened as it usually does in metal but play a central role in the emergence of a series of strange phenomenon such as fractional charge and boundary topological state instead. In this case, our expanding of Hamiltonian by orders is proved to be invalid because of the coupling constant is now large enough to make the series divergence. Wen10 divided the theory in condensed matter physics into two parts: classic ones are those that can be well explained by Ginzberg-Landau theory, the other modern ones has not be well-studies up to now so there is no unified theory describing them. Despite the large amounts of DNN we discussed in this paper that can be well-understanded and even upgraded by Landau-Ginzberg theory, we believe that there still exits some structure of NN that corrsponds to the modern object in condensed matter physics such as topological states and topological orders, and indeed many groups6 are working on the quantum algorithm basing on these strongly-correlated modern concepts. 1.@book{haykin2009neural,title={Neural networks and learning machines},author={Haykin, Simon S},volume={3},year={2009},publisher={Pearson Upper Saddle River, NJ, USA:}} ↩2.@book{peskin1995introduction,title={An introduction to quantum field theory},author={Peskin, Michael Edward},year={1995},publisher={Westview press}} ↩3.@article{Wilson-Renormalization&amp;Critical,title={The renormalization group and critical phenomena},author={Wilson, Kenneth G},journal={Reviews of Modern Physics},volume={55},number={3},pages={583},year={1983},publisher={APS}} ↩4.@book{goodfellow2016deep,title={Deep learning},author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},year={2016},publisher={MIT press}} ↩5.@article{carrasquilla2017machine,title={Machine learning phases of matter},author={Carrasquilla, Juan and Melko, Roger G},journal={Nature Physics},year={2017},publisher={Nature Research}} ↩6.@article{top,title={Exact machine learning topological states},author={Deng, Dong-Ling and Li, Xiaopeng and Sarma, S Das},journal={arXiv preprint arXiv:1609.09060},year={2016}} ↩7.@book{Francesco-CFT,title={Conformal field theory},author={Francesco, Philippe and Mathieu, Pierre and S{‘e}n{‘e}chal, David},year={2012},publisher={Springer Science &amp; Business Media}} ↩8.@book{Kardar-fields,title={Statistical physics of fields},author={Kardar, Mehran},year={2007},publisher={Cambridge University Press}} ↩9.@book{phillips2012advanced,title={Advanced solid state physics},author={Phillips, Philip},year={2012},publisher={Cambridge University Press}} ↩10.@book{Wen-QFT,title={Quantum field theory of many-body systems: from the origin of sound to an origin of light and electrons},author={Wen, Xiao-Gang},year={2004},publisher={Oxford University Press on Demand}} ↩11.@book{nakahara2003geometry,title={Geometry, topology and physics},author={Nakahara, Mikio},year={2003},publisher={CRC Press}} ↩13.@article{why-deep&amp;cheap,title={Why does deep and cheap learning work so well?},author={Lin, Henry W and Tegmark, Max and Rolnick, David},journal={Journal of Statistical Physics},volume={168},number={6},pages={1223–1247},year={2017},publisher={Springer}} ↩","tags":[{"name":"Machine Learing","slug":"Machine-Learing","permalink":"http://yoursite.com/tags/Machine-Learing/"},{"name":"Ginzberg-Landau Theory","slug":"Ginzberg-Landau-Theory","permalink":"http://yoursite.com/tags/Ginzberg-Landau-Theory/"},{"name":"Cluster Expansion","slug":"Cluster-Expansion","permalink":"http://yoursite.com/tags/Cluster-Expansion/"},{"name":"Renormalization Group","slug":"Renormalization-Group","permalink":"http://yoursite.com/tags/Renormalization-Group/"},{"name":"Boltzmann Machine","slug":"Boltzmann-Machine","permalink":"http://yoursite.com/tags/Boltzmann-Machine/"}]},{"title":"Chapter 3: Linear Space Categories","date":"2017-05-29T02:42:21.000Z","path":"2017/05/28/Chapter-3-Linear-Space-Categories/","text":"Chapter 3 studies linear space categories. The free and underlying adjoint construction are still our center of topic, and fortunately the direct sum and tensor product of linear spaces are two extremely apt examples.","tags":[{"name":"Linear Space Category","slug":"Linear-Space-Category","permalink":"http://yoursite.com/tags/Linear-Space-Category/"},{"name":"Free Space","slug":"Free-Space","permalink":"http://yoursite.com/tags/Free-Space/"},{"name":"Direct Sum","slug":"Direct-Sum","permalink":"http://yoursite.com/tags/Direct-Sum/"},{"name":"Tensor Product","slug":"Tensor-Product","permalink":"http://yoursite.com/tags/Tensor-Product/"},{"name":"Free Functors","slug":"Free-Functors","permalink":"http://yoursite.com/tags/Free-Functors/"},{"name":"Underlying Functors","slug":"Underlying-Functors","permalink":"http://yoursite.com/tags/Underlying-Functors/"}]},{"title":"Chapter 2: Group Categories","date":"2017-05-14T13:07:48.000Z","path":"2017/05/14/Chapter-2-Group-Categories/","text":"Chapter 2 studies group categories. We introduce the concepts of free and underlying functors with examples of free group without giving the explicit definition.","tags":[{"name":"Free Functors","slug":"Free-Functors","permalink":"http://yoursite.com/tags/Free-Functors/"},{"name":"Underlying Functors","slug":"Underlying-Functors","permalink":"http://yoursite.com/tags/Underlying-Functors/"},{"name":"Group Category","slug":"Group-Category","permalink":"http://yoursite.com/tags/Group-Category/"},{"name":"Free Group","slug":"Free-Group","permalink":"http://yoursite.com/tags/Free-Group/"}]},{"title":"Chapter 1: Catergories and Functors","date":"2017-04-16T12:40:54.000Z","path":"2017/04/16/Chapter-1-Catergories-and-Functors/","text":"Chapter 1 starts from cateogories and their products and coproducts, then discuss functors and their natural transformations, as well as adjoint ones.","tags":[{"name":"Category","slug":"Category","permalink":"http://yoursite.com/tags/Category/"},{"name":"Functor","slug":"Functor","permalink":"http://yoursite.com/tags/Functor/"},{"name":"Product","slug":"Product","permalink":"http://yoursite.com/tags/Product/"},{"name":"Coproduct","slug":"Coproduct","permalink":"http://yoursite.com/tags/Coproduct/"},{"name":"Natural Transformation","slug":"Natural-Transformation","permalink":"http://yoursite.com/tags/Natural-Transformation/"},{"name":"Adjoint Functors","slug":"Adjoint-Functors","permalink":"http://yoursite.com/tags/Adjoint-Functors/"}]},{"title":"Mathematical Physics: Category Theory","date":"2017-03-20T18:12:05.000Z","path":"2017/03/20/Mathematical-Physics-Category-Theory/","text":"","tags":[{"name":"Preface","slug":"Preface","permalink":"http://yoursite.com/tags/Preface/"}]},{"title":"Mathjax on my Hexo","date":"2016-06-24T05:49:54.000Z","path":"2016/06/24/Mathjax-on-my-Hexo/","text":"Thic page record the configuration of my hexo theme indigo. Configuration FilesRequire Necessary Packages and Define Macros for Convenience and Compatibility1234567891011121314151617181920&lt;% if (theme.mathjax)&#123; %&gt;&lt;!-- mathjax config similar to math.stackexchange --&gt;&lt;script type=\"text/x-mathjax-config\"&gt;MathJax.Hub.Config(&#123; tex2jax: &#123; inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$', '$$'] ], processEscapes: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] &#125;, TeX: &#123; equationNumbers: &#123; autoNumber: \"AMS\" &#125;, extensions: [\"AMSmath.js\", \"AMSsymbols.js\", \"AMScd.js\", \"mediawiki-texvc.js\"], Macros: &#123; bm: \"\\\\boldsymbol\", dd: \"\\\\mathop&#123;&#125;\\\\!\\\\mathrm&#123;d&#125;\", &#125; &#125;&#125;); Add Numbers for Equations1234567MathJax.Hub.Queue(function() &#123; var all = MathJax.Hub.getAllJax(), i; for(i=0; i &lt; all.length; i += 1) &#123; all[i].SourceElement().parentNode.className += ' has-jax'; &#125;&#125;);&lt;/script&gt; Accelate Analysis:12&lt;script async src=\"//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML\" async&gt;&lt;/script&gt;&lt;% &#125; %&gt; Mathjax TestsAMS Font Test1$$\\mathcal&#123;ABC&#125;\\mathbf&#123;ABC&#125;\\mathfrak&#123;ABC&#125;\\mathscr&#123;ABC&#125;\\mathsf&#123;abc&#125;.$$ $$\\mathcal{ABC}\\mathbf{ABC}\\mathfrak{ABC}\\mathscr{ABC}\\mathsf{abc}.$$ Matrix Test1234567$$\\chi(\\lambda) = \\left| \\begin&#123;array&#125;&#123;ccc&#125;\\lambda - a &amp; -b &amp; -c \\\\-d &amp; \\lambda - e &amp; -f \\\\-g &amp; -h &amp; \\lambda - i\\end&#123;array&#125; \\right|.$$ $$\\chi(\\lambda) = \\left| \\begin{array}{ccc}\\lambda - a &amp; -b &amp; -c \\\\-d &amp; \\lambda - e &amp; -f \\\\-g &amp; -h &amp; \\lambda - i\\end{array} \\right|.$$ Commutative Diagrams Test12345$$\\begin&#123;CD&#125;A @&lt;&lt;&lt; B @&gt;&gt;&gt; C \\\\ @. @| @AAA \\\\ @. D @= E.\\end&#123;CD&#125;$$ $$\\begin{CD}A @&lt;&lt;&lt; B @&gt;&gt;&gt; C \\\\ @. @| @AAA \\\\ @. D @= E.\\end{CD}$$","tags":[{"name":"Mathjax","slug":"Mathjax","permalink":"http://yoursite.com/tags/Mathjax/"},{"name":"LaTeX","slug":"LaTeX","permalink":"http://yoursite.com/tags/LaTeX/"},{"name":"Javascript","slug":"Javascript","permalink":"http://yoursite.com/tags/Javascript/"}]},{"title":"Derivation of Einstein's Fields Equation","date":"2016-04-08T22:28:05.000Z","path":"2016/04/08/Einstein Equation-Variation/","text":"We are going to cover two ways of the derivation of Einstein’s equation, one is based on physical analysis, anther one utilizing functional derivatives. Energy-Momentum Tensor Def(Energy-Momentum Tensor):Given the action $I$, the energy momentum tensor of a physical system is defined as $$T_{\\mu\\nu}:=-\\dfrac{2}{\\sqrt{-g}}\\dfrac{\\delta I}{\\delta g^{\\mu\\nu}},$$ where $\\displaystyle\\dfrac{\\delta I}{\\delta g^{\\mu\\nu}}$ is variational derivatives and $g$ represents the determinant of matrix $(g_{\\mu\\nu})$. Example:In electrodynamics, the action is (there may be some typos about the coefficients, but that does not matter at all)$$\\begin{equation}\\label{1.1.1}I_{em}=-\\dfrac{1}{4}\\int d^{4}x\\sqrt{-g}F^{\\mu\\nu}F_{\\mu\\nu}.\\end{equation}$$ If we introduce $\\text{magnetic vector protential }A_{\\mu}$ (It is a valuable to check that this definition automatically satisfies the 4-dimensional Maxell’s equation $\\partial_{[\\alpha}F_{\\mu\\nu]}=0$. Additionally, you can re-express the Maxell’s equation in a more symmetric form by $\\text{Hodge star }*:\\Lambda^{k}(V)\\rightarrow\\Lambda^{n-k}(V)$ in differential manifolds. Here I just skip it because it thouroughly digresses from the topic :) ) $$F_{\\mu\\nu}\\equiv\\partial_{\\mu}A_{\\nu}-\\partial_{\\nu}A_{\\mu}.$$ Then one can show that the energy momentum defined here is compatible with that we already knew in electrodynamics classes: Claim(Energy-Momentum Tensor of Electrodynamics):$$\\begin{equation}\\label{1.1.2}T_{\\rho\\sigma}^{em}=-\\dfrac{2}{\\sqrt{-g}}\\dfrac{\\delta I_{em}}{\\delta g^{\\rho\\sigma}}=-\\dfrac{1}{4}g_{\\rho\\sigma}F^{2}+F_{\\rho\\nu}F_{\\sigma}^{~\\nu}.\\end{equation}$$ Proof:(skipped) Derivation Based on Functional Derivatives Undoubtedly it is based on talent analyses on physics that Einstein firstly write down the Field Equation. The reason why I give you another approach at first is because of some pedagogical consideration. Axiom:(Hilbert Action)The action of general relativity is given by Hilbert that $$I_{G}=\\dfrac{1}{16\\pi G}\\int dx^{4}\\sqrt{-g}R,$$ where $R$ is Ricci scalar. Proposition(Einstein’s Field Equation):$$\\begin{equation}\\label{1.2.1}R_{\\mu\\nu}-\\dfrac{1}{2}g_{\\mu\\nu}R=8\\pi GT_{\\mu\\nu}\\end{equation}$$ Proof:$$\\delta I_{G}=\\dfrac{1}{16\\pi G}\\int dx^{4}\\bigg[(\\delta\\sqrt{-g})g^{\\mu\\nu}R_{\\mu\\nu}+\\sqrt{-g}(\\delta g^{\\mu\\nu})R_{\\mu\\nu}+\\sqrt{-g}g^{\\mu\\nu}\\delta R_{\\mu\\nu}\\bigg].$$ There is no harm to consider the derivation in the local inertial coordinate(think why?), that is, $g_{\\mu\\nu}\\rightarrow\\eta_{\\mu\\nu}$ and $\\varGamma_{\\mu\\nu}^{\\sigma}=0$. Thus,$$\\delta R_{\\mu k}=\\partial_{k}\\delta\\varGamma_{\\mu\\lambda}^{\\lambda}-\\partial_{\\lambda}\\delta\\varGamma_{\\mu k}^{\\lambda}+0+0-0-0.$$ And it is easy to check that $\\delta\\varGamma$ is always a tensor.(verify!) (Hint: Consider two distinct connections defined on the manifold and their affine connection’s behaviors under the coordinate tansformation.) In this way,$$\\begin{align}\\sqrt{-g}g^{\\mu\\nu}R_{\\mu\\nu}&amp;=\\sqrt{-g}g^{\\mu\\nu}(\\partial_{\\nu}\\delta\\varGamma_{\\mu\\lambda}^{\\lambda})-\\sqrt{-g}g^{\\mu\\nu}(\\partial_{\\lambda}\\delta\\varGamma_{\\mu\\nu}^{\\lambda})\\nonumber\\\\=&amp;\\partial_{\\nu}(\\sqrt{-g}g^{\\mu\\nu}\\delta\\varGamma_{\\mu\\lambda}^{\\lambda})-\\partial_{\\lambda}(\\sqrt{-g}g^{\\mu\\nu}\\delta\\varGamma_{\\mu\\nu}^{\\lambda}),\\end{align}$$which vanishes at the boundary of spacetime by integrating by parts. So$$\\begin{align}\\delta I_{G}&amp;=\\dfrac{1}{16\\pi G}\\int dx^{4}\\bigg[(\\delta\\sqrt{-g})g^{\\mu\\nu}R_{\\mu\\nu}+\\sqrt{-g}(\\delta g^{\\mu\\nu})R_{\\mu\\nu}\\bigg]\\nonumber\\\\&amp;=\\dfrac{1}{16\\pi G}\\int dx^{4}\\left[\\dfrac{\\sqrt{-g}}{2}g^{\\rho\\sigma}g^{\\mu\\nu}R_{\\mu\\nu}\\delta g_{\\rho\\sigma}+\\sqrt{-g}(-g^{\\mu\\rho}g^{\\nu\\sigma}\\delta g_{\\rho\\sigma})R_{\\mu\\nu}\\right]\\nonumber\\\\&amp;=\\dfrac{1}{16\\pi G}\\int dx^{4}\\sqrt{-g}\\delta g_{\\rho\\sigma}\\left(\\dfrac{1}{2}g^{\\rho\\sigma}R-R^{\\rho\\sigma}\\right),\\nonumber\\end{align}$$or (there are some nontivial indice contraction tricks in the above computation that I believe you can handle, so I just left them as exercises.) $$\\displaystyle\\dfrac{\\delta I_{G}}{\\delta g^{\\rho\\sigma}}=\\dfrac{\\sqrt{-g}}{16\\pi G}\\left(-\\dfrac{1}{2}g^{\\rho\\sigma}R-R^{\\rho\\sigma}\\right).$$ On the other aspect, $\\displaystyle\\dfrac{\\delta I_{G}}{\\delta g^{\\rho\\sigma}}=-\\dfrac{\\sqrt{-g}}{2}T^{\\rho\\sigma}$. At last, by reducing the indices we get $\\text{Einstein’s Field Equation}$: $$\\begin{equation}R_{\\mu\\nu}-\\dfrac{1}{2}g_{\\mu\\nu}R=8\\pi GT_{\\mu\\nu}.\\end{equation}$$","tags":[{"name":"Newtonian Limits","slug":"Newtonian-Limits","permalink":"http://yoursite.com/tags/Newtonian-Limits/"},{"name":"Variational Methods","slug":"Variational-Methods","permalink":"http://yoursite.com/tags/Variational-Methods/"}]},{"title":"Introduction to Homology Group","date":"2016-04-05T00:26:35.000Z","path":"2016/04/04/Introduction-to-Homology-Group/","text":"To have a quick view of homology, I’ll use non-formal language to introduce homology group and then take $T^{2}$ as an simple example to illustrate the motivation of its definition and construction. Let’s take a look at $S^{2}$ and $T^{2}$. Triangulation of $S^2$ Triangulation of $T^2$ It’s obvious that the topological structure of these two manifolds are essentially different. On the surface, $S^{2}$ has no hole while $T^{2}$ has one. However, we can mathematically reinterpret this difference as that any circle $S^{1}$ on the $S^{2}$ encloses a domain, that is, $\\forall S^{1}\\subset S^{2},\\exists \\text{close disk }B\\subset S^{2} \\text{ s.t. } S^{1}=\\partial B$. On the contrary, there are two kinds of circle on $T^{2}$ that do not enclose any domain: latitude and longitude circle.(One incision but two boundaries) Through closely observation one can find that although one circle(either latitude or longitude one) cannot enclose a domain, two circle of the same type can do. What does this mean? Two longitude(or latitude) circle has no difference on reflecting topological structure of $T^{2}$, in other words, they belongs to the same class. Moreover, any longitude circle and latitude circle cannot enclose a domain, so they belongs to two different class. In mathematics, this compact(closed) class of sub-manifolds which does not enclose a domain and can be orientated are called ??? ( 英语不会。。可定向无边紧子流形类, there is no harm that I personally call this as OSCNB). They are exactly the generators of homology groups. Take $T^{2}$ for example, zero-dimension OSCNB is just the class $[p]$ that is generated by one point $p\\in T^{2}$. Because any distinct point can enclose a segment on $T^{2}$, there are two kinds of one-dimensional OSCNB, as is discussed before(latitude and longitude ones). Additionally, $T^{2}$ itself is a two-dimensional OSCNB. In this way, k-dimensional homology group of $T^{2}$, denoted as $H_{k}(T^{2},\\mathbb{R})$, is a group showing as follow: $$\\begin{align}H_{0}(T^{2},\\mathbb{R})&amp;=\\{\\alpha e_{0}|\\alpha\\in\\mathbb{R}, e_{0}=[p]\\}\\cong\\mathbb{R},\\nonumber\\\\H_{1}(T^{2},\\mathbb{R})&amp;=\\{\\alpha_{1}e_{1}+\\alpha_{2}e_{2}|\\alpha_{i}\\in\\mathbb{R}, e_{i}=[\\sigma_{i}], i=1,2\\}\\cong\\mathbb{R}^{2},\\nonumber\\\\H_{2}(T^{2},\\mathbb{R})&amp;=\\{\\alpha e_{2}|\\alpha\\in\\mathbb{R}, e_{2}=[T^{2}]\\}\\cong\\mathbb{R},\\nonumber\\end{align}$$ Here, $[\\sigma_{1}]$ denotes class of longitude circles, while $[\\sigma_{2}]$ denotes class of latitude circles. In the same way, k-dimensional homology group of $S^{2}$ is $$\\begin{align}H_{0}(S^{2},\\mathbb{R})&amp;=\\{\\alpha e_{0}|\\alpha\\in\\mathbb{R}, e_{0}=[p]\\}\\cong\\mathbb{R},\\nonumber\\\\H_{1}(S^{2},\\mathbb{R})&amp;=0, \\nonumber \\\\H_{2}(S^{2},\\mathbb{R})&amp;=\\{\\alpha e_{2}|\\alpha\\in\\mathbb{R}, e_{2}=[S^{2}]\\}\\cong\\mathbb{R}.\\nonumber\\end{align}$$ In fact, for general n-dimensional compact manifolds $M$, its homology group is that which takes its OSCNB as generators and a commutative addition group $G$ as its coefficients. For instance, if $G=\\mathbb{R}$, all k-dimensional homology group with real coefficients are $$\\begin{align}H_{k}(M,\\mathbb{R})&amp;=\\left\\{\\left.\\sum_{j=1}^{m_{k}}\\alpha_{j}e_{j}^{k}\\right|\\alpha_{j}\\in\\mathbb{R}, 1\\leqslant j\\leqslant m_{k}\\right\\}\\cong\\mathbb{R},\\quad 0\\leqslant k\\leqslant n,\\nonumber\\\\H_{k}(M,\\mathbb{R})&amp;=0,\\quad\\forall k&gt;n\\nonumber\\end{align}$$ The original purpose of constructing Homology group is to find topological invariants in topological space(to prove this we should show that homology groups defined here is invariant under partition, and that’s another topic), that is, two homeomorphism topological space must have isomorphism homology group. At this point can we safely conclude that $S^{2}$ and $T^{2}$ are topological distinct because their one-dimensional homology groups are different !","tags":[{"name":"Homology Goup","slug":"Homology-Goup","permalink":"http://yoursite.com/tags/Homology-Goup/"},{"name":"Mathematica","slug":"Mathematica","permalink":"http://yoursite.com/tags/Mathematica/"}]},{"title":"临江仙·加冠","date":"2016-03-26T00:52:40.000Z","path":"2016/03/25/临江仙·加冠/","text":"《礼记•曲礼上》“男子二十，冠而字”。三月廿四，仆填词以自述。 弱冠表德[1]赐剑履[2]，也羡千里豪行[3]。穷哭[3]广武楚魂听[5]。东军[6]陷漠北[7]，传世飞将[8]名。 吟哦三余[9]犯雕辇[10]，才得推敲妙语[11]。未妨柯烂[12]斧还新。授弈仙子[13]路[14]，负手[14]制纹枰[15]。 【注】 [1]弱冠：《礼记 曲礼上》：「男子二十，冠而字」；表德：《颜氏家训 风操》「古者，名以正体，字以表德」[2]剑履：语出《三国志 曹真传》「四年，朝洛阳，迁大司马，赐剑履上殿，入超不趋」[3]千里豪行：李白《侠客行》「十步杀一人，千里不留行」。此处所指，虽今日加冠佩剑，亦不忘太白游侠之想。[4]穷哭：《晋书 卷四九》（阮籍）「时率意独驾，不由径路，车迹所穷，辙恸哭而反」[5]广武楚魂：同《晋书 卷四九》（阮籍）「尝登广武山，观楚汉战处，叹曰‘时无英雄，使竖子成名’」[6]东军：指李广部众。《史记 李广列传》「广不谢大将军而起行，意甚愠而就部，引兵与右将军食其合军而出东道」[7]陷漠北：漠北之战，李广怒出东道，军亡导，或失道，而后大将军卫青。后自愧难对刀笔吏而自刎。[8]飞将：李广。[9]三余：本出自《三国志 魏书 钟繇华歆王朗传》「冬者，岁之余也；夜者，日之余也；阴雨者，时之余也」，意味好学，本处形容吟哦废寝忘食之态。亦可简单作虚词解。[10]推敲妙语：贾岛炼字忘神，倒骑驴而犯韩愈车辇，得「僧敲月下门」之佳句。[11]柯：斧柄。[12]受益仙子：任昉《述异记》「晋时王质伐木至，见童子数人棋而歌，质因听之。童子以一物与质，如枣核，质含之而不觉饥。俄顷，童子谓曰：‘何不去？’质起视，斧柯尽烂，既归，无复时人」，古多用于慨叹恍若隔世感，此处用其观棋入定忘神意。[13]路：棋道。[14]负手：背着手。指棋艺大成，已有近世吴清源让天下一先之力。 【释】 既加冠，宜为大人事，首句却反道而行，先表赤子之心：冠弱未稳，剑法生疏，或得执剑上殿，亦不忘少年游侠痴梦，可谓中二依旧也。而前路未可知，或有穷途难行之时，任如阮嗣宗般哭天泣地，也仅有楚魂来听罢。然则，李广失路漠北，不过几日迟后，而军情不误。飞将之名，无有损益！ 贾岛撞辇推敲，王质观棋授弈，均有所获，盖大道归一，本无所谓功成之正途。凡至极处，皆可大成。曷不不舍而锲之？","tags":[{"name":"宋词","slug":"宋词","permalink":"http://yoursite.com/tags/宋词/"}]},{"title":"Unifying the Annoying Notation of Riemann Curvature Tensor","date":"2016-03-26T00:20:09.000Z","path":"2016/03/25/Unifying-the-Annoying-Notation-of-Riemann-Curvature-Tensor/","text":"Having had enough of the different forms of Riemann Curvature Tensor, I finally decided to recommend the “most correct” notation of Riemann Curvature here. In mathematics books, we say $\\text{Operator }R:\\Gamma(TM)\\times\\Gamma(TM)\\times\\Gamma(TM)\\rightarrow\\Gamma(TM)$ is $\\text{the curvature of connection } D$ (Here connection is a general one which may not be torsion-free), if it satisfies: $$R(X,Y)Z=D_{X}D_{Y}Z-D_{Y}D_{X}Z-D_{[X,Y]}Z.$$ In the definition of $\\text{Affine Connection Coefficient}$, we have $D_{e_{i}}e_{j}=\\Gamma_{ij}^{k}e_{k}$ for any tangent frame field $e_{1},\\cdots,e_{m}$, thus \\begin{align}R\\left(\\dfrac{\\partial}{\\partial x^{i}},\\dfrac{\\partial}{\\partial x^{j}}\\right)\\dfrac{\\partial}{\\partial x^{k}}&amp;=:R_{kij}^{l}\\dfrac{\\partial}{\\partial x^{l}}\\nonumber\\\\&amp;=D_{\\partial/\\partial x^{i}}\\left(\\Gamma_{jk}^{l}\\dfrac{\\partial}{\\partial x^{l}}\\right)-D_{\\partial/\\partial x^{j}}\\left(\\Gamma_{ik}^{l}\\dfrac{\\partial}{\\partial x^{l}}\\right)\\nonumber\\\\&amp;=\\left(\\partial_{i}\\Gamma_{jk}^{l}-\\partial_{j}\\Gamma_{ik}^{l}\\right)\\dfrac{\\partial}{\\partial x^{l}}+\\Gamma_{jk}^{m}\\Gamma_{im}^{n}\\delta_{n}^{l}\\dfrac{\\partial}{\\partial x^{l}}-\\Gamma_{ik}^{m}\\Gamma_{jm}^{n}\\delta_{n}^{l}\\dfrac{\\partial}{\\partial x^{l}}\\nonumber\\\\&amp;=\\left(\\partial_{i}\\Gamma_{jk}^{l}-\\partial_{j}\\Gamma_{ik}^{l}+\\Gamma_{jk}^{m}\\Gamma_{im}^{l}-\\Gamma_{ik}^{m}\\Gamma_{jm}^{l}\\right)\\dfrac{\\partial}{\\partial x^{l}}\\end{align} Here I list some typically torion-free Reimann Curvature Tensor for readers to compare and memorize: Lu(卢建新) &amp; Chen(陈维恒): $$R_{\\text{(陈.) }kij}^{l}=\\partial_{i}\\Gamma_{kj}^{i}-\\partial_{j}\\Gamma_{ki}^{l}+\\Gamma_{kj}^{h}\\Gamma_{kh}^{l}-\\Gamma_{ki}^{h}\\Gamma_{kj}^{l}.$$ S.Weinberg: $$R_{\\text{(W.) }\\mu\\nu k}^{\\lambda}=-R_{\\text{(陈.) }\\mu\\nu k}^{\\lambda}$$","tags":[{"name":"Riemann Curvature","slug":"Riemann-Curvature","permalink":"http://yoursite.com/tags/Riemann-Curvature/"},{"name":"Notation","slug":"Notation","permalink":"http://yoursite.com/tags/Notation/"}]},{"title":"Noether Theorem for Non-autonomous System","date":"2016-02-28T23:11:44.000Z","path":"2016/02/28/Noether-Theorem-for-Non-autonomous-System/","text":"鉴于大多数物理书混淆概念，把 $\\text{Noether}$ 定理讲得天花乱坠还无比繁杂，这里给出真正正确清晰的定理表述，以正视听。 自治系统是下面非自治系统的一般情况，或参见[1]. 以下约定记号: 下标为一的是非自治系统物理量, 无下标为自治系统的. 自治系统 $\\text{Lagrangian}$ 含时, 将时间并入广义坐标, 引入参数 $\\tau$, 使得真实运动 $x^{\\mu}=\\varphi(\\tau)$ (约定 $x^{0}\\equiv t$, $\\mu=1,2,3$. 位形流形 $M_{1}=M\\times\\mathbb{R}$. 定义 1：位形流形 $M_1$ 上的拉氏量 $\\mathcal{L}_1:TM\\rightarrow\\mathbb{R}$ 定义为$$\\mathcal{L}_1\\left(t,\\bm{q},\\dfrac{\\partial t}{\\partial\\tau},\\dfrac{\\partial\\bm{q}}{\\partial\\tau}\\right):=\\mathcal{L}\\left(t,\\dfrac{\\partial\\bm{q}/\\partial\\tau}{\\partial t/\\partial\\tau},\\bm{q}\\right)\\dfrac{\\rm{d}t}{\\rm{d}\\tau}.$$ 定义 2：称光滑映射 $h$ 为容许映射, 若满足推前映射下拉式量不变, 即 $$\\displaystyle\\forall \\bm{v}\\in TM_{1}, \\mathcal{L}_1(h_{*}\\bm{v})=\\mathcal{L}_1(\\bm{v})$$. 定理 1（Noether）：若系统 $(M_1,\\mathcal{L}_1)$ 容许单参微分同胚映射 $h^{s}:M_1\\rightarrow M_1$, 则存在首次积分$$\\begin{equation}\\displaystyle I_{1}=\\dfrac{\\partial\\mathcal{L}_{1}}{\\partial\\left(\\dfrac{\\mathrm{d} x^{\\mu}}{\\mathrm{d}\\tau}\\right)}\\cdot\\dfrac{\\partial h^{s}(x^{\\mu})}{\\partial s}.\\end{equation}$$ 证明：如前, 记 $x^{\\mu}=\\varphi(\\tau)$ 为真实运动, 满足 E-L 方程. 依容许映射定义, 容许映射保持拉氏量不变, 故 $h^{s}x^{\\mu}=:\\Phi(\\tau,s)$ 也满足 E-L 方程. 拉式量不变意为$$0\\equiv\\dfrac{\\partial\\mathcal{L}_{1}}{\\partial s}=\\dfrac{\\partial\\mathcal{L}_{1}}{\\partial x^{\\mu}}\\cdot\\dfrac{\\partial\\Phi}{\\partial s}+\\dfrac{\\partial\\mathcal{L}_{1}}{\\partial\\left(\\dfrac{\\mathrm{d}x^{\\mu}}{\\mathrm{d}\\tau}\\right)}\\cdot\\dfrac{\\partial}{\\partial s}\\left(\\dfrac{\\mathrm{d}\\Phi}{\\mathrm{d}\\tau}\\right).$$改 $\\tau$ 为参数后, E-L 方程成为$$\\dfrac{\\mathrm{d}}{\\mathrm{d}\\tau}\\dfrac{\\partial\\mathcal{L}_{1}}{\\partial\\left(\\dfrac{\\mathrm{d}x^{\\mu}}{\\mathrm{d}\\tau}\\right)}=\\dfrac{\\mathcal{L}_{1}}{\\partial x^{\\mu}}.$$ 推论 1：首次积分可以明显写为$$\\begin{equation}\\displaystyle I_{1}=-\\mathcal{H}\\dfrac{h^{s}(t)}{\\partial s}+\\dfrac{\\partial\\mathcal{L}}{\\partial\\dot{\\bm{q}}}\\cdot\\dfrac{h^{s}(\\bm{q})}{\\partial s}.\\end{equation}$$ 证明：对前半项的 $x^{0}$ 分量, 有$$\\begin{align}\\dfrac{\\partial\\mathcal{L}_{1}}{\\partial\\left(\\dfrac{\\mathrm{d} x^{0}}{\\mathrm{ d}\\tau}\\right)} &amp;= \\dfrac{\\partial}{\\partial\\left(\\dfrac{\\mathrm{d}x^0}{\\mathrm{d}\\tau}\\right)}\\left(\\mathcal{L}\\left(t,\\dfrac{\\mathrm{d}\\bm{q}/\\mathrm{d}\\tau}{\\mathrm{d}t/\\mathrm{d}\\tau}\\right)\\dfrac{\\mathrm{d}t}{\\mathrm{d}\\tau}\\right)\\nonumber\\\\&amp;=\\mathcal{L}+\\left[\\dfrac{\\partial\\mathcal{L}}{\\partial\\dot{\\bm{q}}}\\cdot\\left(-\\dfrac{\\mathrm{d}\\bm{q}/\\mathrm{d}\\tau}{(\\mathrm{d}t/\\mathrm{d}\\tau)^{2}}\\right)\\right]\\cdot\\dfrac{\\mathrm{d}t}{\\mathrm{d}\\tau}\\nonumber\\\\&amp;=\\mathcal{L}-\\dot{\\bm{q}}\\dfrac{\\partial\\mathcal{L}}{\\partial\\dot{\\bm{q}}}=\\mathcal{H}.\\nonumber\\end{align}$$其余同理, 留作练习. 参考文献： [1] Anold, 经典力学中的数学方法. 高教社.","tags":[{"name":"Conservation Current","slug":"Conservation-Current","permalink":"http://yoursite.com/tags/Conservation-Current/"},{"name":"Noether Theorem","slug":"Noether-Theorem","permalink":"http://yoursite.com/tags/Noether-Theorem/"},{"name":"Symmetry","slug":"Symmetry","permalink":"http://yoursite.com/tags/Symmetry/"}]},{"title":"Hello World","date":"2016-01-03T01:23:31.000Z","path":"2016/01/02/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment Syntax TestFootnoteI love you1 1.test ↩","tags":[]}]