<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>逸世凌虚的试剑亭</title>
  
  <subtitle>云行雨施，品物流形</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-11-19T14:51:51.556Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Xiaodong Hu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Landau-Ginzberg Effective Theory on Deep Neural Network</title>
    <link href="http://yoursite.com/2017/11/02/Landau-Ginzberg-Effective-Theory-on-Deep-Neural-Network/"/>
    <id>http://yoursite.com/2017/11/02/Landau-Ginzberg-Effective-Theory-on-Deep-Neural-Network/</id>
    <published>2017-11-01T18:14:27.000Z</published>
    <updated>2017-11-19T14:51:51.556Z</updated>
    
    <content type="html"><![CDATA[<p>Deep neural network (DNN) works in an amazing effeciency in machine learing but cannot be well-explained by accurate mathematics. Reviewing the one-to-one correspondence between Restricted Boltzmann machines (RBM) and configuration space renomalization groups (RG), the well-developed techniques in studying quantum fields theory (QFT), we are motivated to apply the effective fields theory, particularly Landau-Ginzberg one, to explain the expressibility and mechnism of a general DNN. Similar structures of the effective Hamiltionian are discovered, and we propose a new model based on the systematic cluster expansion approach to improve the performance of DNN without adding extra layer. Supported by the high efficiency in counting low orders Feynman diagrams in computation of scattering amplitute by QFT, the upgrade of DNN we propose is believed to provide a higher efficiency than extending the depth of DNN if the coarse graining procedure has neared the fixed point of RG flows. </p><a id="more"></a><h1 id="Exact-Representation-of-Physical-Models"><a href="#Exact-Representation-of-Physical-Models" class="headerlink" title="Exact Representation of Physical Models"></a>Exact Representation of Physical Models</h1><h2 id="Renormalization-Group-of-Ising-Model"><a href="#Renormalization-Group-of-Ising-Model" class="headerlink" title="Renormalization Group of Ising Model"></a>Renormalization Group of Ising Model</h2><p>The <em>Boltzman machine</em> in ML is a stochastic binary whose composition consists of stochastic neurons<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>. Coincidentally, in physics we have a large fields of binary-valued spin models on lattice to explain the wide properties of magnetism. A typical and famous one is the <em>Ising model</em> . Ising model is a $N$ spin qubit system dominated by the exchange interaction (here we consider the general case)<br>$$H(\{s\},\bm{K})=-\sum_iK_i s_i-\sum_{ij}K_{ij}s_i s_j-\cdots.$$<br>By statistical theory of canonical ensemble, we have the partition function $Z\equiv\mathrm{Tr}\,\hat{\rho}=\mathrm{Tr}\,e^{-\beta \hat{H}(\{s\})}$ and the free energy $F=-kT\ln Z$.</p><p>Physicist has developed a systematic and powerful technique called <em>renormalization group</em> to describe the distinct behaviors of fields theory on different energy levels<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>, which was then generalized to the change of lengh scale as well and perfectly handled the description of divergence beahavior near critical point of phase transitions<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>. The heart of the point is, when we alter the scale of an arbitrary system, the physical laws behind it still holds without any change of form. To put it more explicitly, we take the Ising model here as one example, as is shown in <strong>fig:1</strong></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://wx4.sinaimg.cn/large/0060CjOygy1flnpp1giw5j30ff0g2408.jpg" alt="**Coarse graining process of a $8imes8$ lattice**. In each step of rescaling (illustrated by different colors of dashed squares), spins in each block is averaged and renormalized but holds the configuration of the square lattice." title="">                </div>                <div class="image-caption">**Coarse graining process of a $8imes8$ lattice**. In each step of rescaling (illustrated by different colors of dashed squares), spins in each block is averaged and renormalized but holds the configuration of the square lattice.</div>            </figure><p>Though introducing new blocks of spins and average as well as normalize them, i.e., $\Sigma_I=\frac{1}{R}\sum_{i\in I}s_i$, where $R$ is a normalize factor to make $\Sigma_I$ still takes value of either $-1$ or $+1$, the details of initial spins distribution are cancelled out and we are left with the inner structure of the system. An this process is called \emph{coarse graining}.\par<br>Denote $\{s\}$ as the initial spin distribution, $\{s’\}$ the next coarse grained ones and $\lambda$ the scaling factor, by the <strong>scaling hypothesis</strong><sup id="fnref:7"><a href="#fn:7" rel="footnote">7</a></sup>, we still have<br>$$H^{RG}_\lambda(\{s’\},\bm{K’})=-\sum_iK’_i s’_i-\sum_{ij}K’_{ij}s’_i s’_j-\cdots,$$<br>where $\bm{K’}$ describes the new interaction between coarse grained spins, or hidden spins. In mathematics literature, mapping $f_\lambda:\bm{K}\rightarrow\bm{K’}$ forms a semigroup action since it is irreversible, and we call $\{f\}$ the RG group. If the GR-group flow reach the fixed point in the parameter space[^Francesco-CFT]. That is,<br>$$\bm{K_c}=\bm{f}(K_c),$$<br>then the difference of free energy of two coarse graining process will vanish<br>$$\Delta F=0\implies\mathrm{Tr}\,e^{-\beta T_\lambda(\{s\},\{s’\})}=0,$$<br>where $T_\lambda(\{s\},\{s’\})\equiv H(\{s\},\bm{K})-H_\lambda^{RG}(\{s’\},\bm{K’})$.</p><hr><p>Next, we are to map RG to the <strong>Restriced Boltzman Machine(RBM)</strong>. The stochasitic neurons of the RBM are partitioned into two groups, <strong>visible neurons</strong> and <strong>hidden neurons</strong>. Visible neurons provide an interface between the network and the ambient environment and during the training process they are all clumped onto specific states determined by the environment. The hidden neurons, however, always operate freely that they are used to explain the underlying constraints  contained in the environmental input vectors<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup>.<br>Coupling between one layer to the next coarse grained layer is characterized by the operator $T_\lambda(\{s\},\{s’\})$, in RBM, in fact, we also has a analogous object called joint energy between hidden neurons and visible neurons<br>$$\begin{equation}\label{2.1}<br>    E(\{v_i\},\{h_j\})=\sum_jb_jh_j+\sum_{ij}v_iw_ijb_j+\sum_ic_iv_j,<br>\end{equation}$$<br>where $\{v\}$ denote the visible unit while $\{h_i\}$ denote the hidden unit.</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://wx1.sinaimg.cn/large/0060CjOygy1flnpufbfkqj30hx0duq4o.jpg" alt="**Restricted Boltzman Machine**. The connections between the visible and hidden neurons are symmetric. Unlike general BM, such symmetric connections do not extend to the visible and hidden neurons." title="">                </div>                <div class="image-caption">**Restricted Boltzman Machine**. The connections between the visible and hidden neurons are symmetric. Unlike general BM, such symmetric connections do not extend to the visible and hidden neurons.</div>            </figure><p>Similarly we can define the joint probability distribution<br>$$\begin{equation}\label{2.2}<br>    p_\lambda(\{v\},\{h\})=\dfrac{1}{Z}e^{-E(\{v\},\{h\})}<br>\end{equation}$$<br>and variational distribution for visible neurons as well<br>$$\begin{equation}\label{2.3}<br>    p_\lambda(\{v\})\equiv\sum_{\{h\}}p_\lambda(\{v\},\{h\})=\mathrm{Tr}_{h}\,p_\lambda(\{v\},\{h\}).<br>\end{equation}$$<br>And the training tast is to minimize the Kullback-Leibler divergence between the visible distribution to the true one<br>$$\begin{equation}\label{2.4}<br>    D_{KL}\big(P(\{c\})\mid\mid p_\lambda(\{v\})\big)=\sum_{\{v\}}P(\{v\})\ln\left(\dfrac{P(\{v\})}{p_\lambda(\{v\})}\right).<br>\end{equation}$$<br>Coupling between coarse grained states are encoded by the operator $T_\lambda(\{v\},\{h\})$, while in RBM an analogous role is played by the joint energy $E(\{v\},\{h\})$. In fact, equation<br>$$\begin{equation}\label{2.5}<br>    T(\{v\},\{h\})=-E(\{v\},\{h\})+H[\{v\}].<br>\end{equation}$$<br>one-to-one maps the RG scheme to deep RBM, as is proven below.\par<br>By definition of $T$, we have<br>$$\dfrac{1}{Z}e^{-H_\lambda^{RG}[\{h\}]}=\dfrac{1}{Z}\mathrm{Tr}_v\,e^{T_\lambda(\{v\},\{h\})-H[\{v\}]}.$$<br>Substituting the claimed Eq. $\eqref{2.5}$, one immediately gets<br>$$H_\lambda^{RG}[\{h\}]=H_\lambda^{\text{RBM}}[\{h\}]$$<br>and we are done.</p><p>Since $\lambda$ in RBM indicates the two pair of hidden and visible layers, by this proposition we can make a corollary that the layer of DNN plays the same roles of RG flows reaching the fixed point. And this also explains why the depth of the NN counts.</p><h1 id="Understanding-of-Neural-Network-with-Points-of-Effective-Fields-Theory"><a href="#Understanding-of-Neural-Network-with-Points-of-Effective-Fields-Theory" class="headerlink" title="Understanding of Neural Network with Points of Effective Fields Theory"></a>Understanding of Neural Network with Points of Effective Fields Theory</h1><p>In an arbitrary task of prediction in machine learning, probability distribution $p(y|x)$ is the crux we are concerned about, where $x$ is the vector in sampling space and $y$ the model variables. Explicitly, in a typical classification problem of aninmals, we interpret $x$ as features of testing pictures, such as vectors taking values in $\{\text{fur}, \text{whikser}, \text{eye},\cdots\}$ and $y$ as animals $\{\text{cat}, \text{dog}, \text{human}, \cdots\}$ we want to classify. As a physical example, we may interpret $x$ as the input spin configuration and $y$ the set of physical phases such as high/low temperature phases<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup> and long-length entangled topological states<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup>.<br>Anyway, by Bayer’s theorem, we always have<br>$$p(y|x)=\dfrac{p(x|y)p(y)}{\displaystyle\sum_{y’}p(x|y’)p(y’)}.$$<br>If we endow the two negative logarithms with physical meanings as following:<br>$$\begin{align}<br>H_y(x)&amp;:=-\ln p(x|y),\label{3.1}\\<br>\mu_y&amp;:=-\ln p(y),\label{3.2}<br>\end{align}$$<br>then Bayer’s theorem can be expressed as a more familiar form in statistical physics<br>$$\begin{equation}\label{3.3}<br>    p(y|x)=\dfrac{1}{Z(x)}e^{-(H_y(x)-\mu_y)},<br>\end{equation}$$<br>where partition function<br>$$Z(x)\equiv\displaystyle\sum_y e^{-(H_y(x)-\mu_y)}.$$<br>where $H_y$ is Hamiltonian and $\mu_y$ the chemical potential of the physical system.</p><p>The recasting of conditional probability by Hamiltonian and introducing of chemical potential is NOT futile as it seems. One the one hand, the task of finding an arbitrary function of our system was converted to find a quantity of rich physical meaning, and then we can understand the mechinism of neuron network in an thoroughly different perspective and may even discern some inner structure of NN according to the propeties of Hamiltonian that is widely used in theoretical physics but not revealed by experts in ML yet.</p><h2 id="Landau-Ginzberg-Theory"><a href="#Landau-Ginzberg-Theory" class="headerlink" title="Landau-Ginzberg Theory"></a>Landau-Ginzberg Theory</h2><p>Let us start with reviewing the <strong>Landau-Ginzberg fields</strong> in effective theory of statistical fields. Through introducing variables under <strong>mesoscopic</strong> scales under the assumption<sup id="fnref:8"><a href="#fn:8" rel="footnote">8</a></sup> that $\lambda\gg\dd x\gg a$, the partition function of microscopic states can always be re-expressed by the coarse grained mesoscopic fields<br>$$\begin{equation}\label{3.4}<br>    Z(T)=\mathrm{Tr}\,e^{-\beta H_{\text{micro}}}=\int{\mathcal D}\psi\,\mathcal{W}[\psi(\bm{x})],<br>\end{equation}$$<br>where the measure $\mathcal{D}\psi$ in Feynman path integral[^nakahara2003geometry] runs over all the possible configuration of fields in the function space and $\mathcal{W}[\psi]$ is the weights of each configuration. Although obtaining the precise form of $\mathcal{W}[\psi]$ is actually not easier than exactly solving the full problem with accurate partition function, is is possible to describe it in terms of only a few <strong>phenominological parameters</strong> but not a large number of microscopic canonical variables. If we define the effective Hamiltonian of the system by<br>$$\beta H[\psi(\bm{x})]:=-\ln\mathcal{W}[\psi(\bm{x})],$$<br>then it can be expanded to a general form that<br>$$\begin{equation}\label{3.5}<br>    \beta H=\int \dd\bm{x}~\Phi[\bm{x},\psi(\bm{x}),\nabla\psi(\bm{x}),\nabla^2\psi(\bm{x}),\cdots].<br>\end{equation}$$<br>It seems that what we have done is merely equivalent mathematical transformation, but since now all the variable in our Hamiltonion are mesoscopic ones, we can apply the symmetries of system or impose extra demands of our desired theory such as locality and casuality. These simple consideration will dramatically simplify the above complicated expression. \par<br>In condensed matter physics, solid lattice are ususually regared as invariant under translation (temporarilly neglect all the defects), so does the explicit form of $\Phi$. So<br>$$\Phi[\bm{x},\psi(\bm{x}),\nabla\psi(\bm{x}),\cdots]=\Phi[\psi(\bm{x}),\nabla\psi(\bm{x}),\cdots].$$<br>Likewise, lattice are also invariant under rotations, dropping all the odd power of gredient of fields operators:<br>$$\begin{align}<br>    \beta H&amp;=\int\dd\bm{x}~\Phi[\psi(\bm{x})^2,\nabla^2\psi(\bm{x}),\cdots]\\<br>    &amp;=\int\dd\bm{x}~K_1\psi^2+K_2\psi^4+\cdots+M_1(\nabla\psi)^2+M_2(\nabla\psi)^4\nonumber\\<br>    &amp;\qquad+\cdots+N_1\psi^2(\nabla\psi)^2+N_2\psi^4(\nabla\psi)^4+\cdots.\nonumber<br>\end{align}$$<br>Most importantly, in many cases involving merely short-range interactions (even long-range interaction such as Coulumb one becomes short-range due to the screening effect<sup id="fnref:9"><a href="#fn:9" rel="footnote">9</a></sup>), locality guarantees that only the lower order terms make remarkable contribution in Hamiltonian, so we are finally left with<br>$$\begin{equation}\label{3.6}<br>    \beta H=\int\dd\bm{x}~\bigg(K_1\psi^2+K_2(\nabla\psi)^2\bigg)+\text{interactive perturbation},<br>\end{equation}$$<br>where contribution from interaction can be carefully analysis by counting Feynman diagrams<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> (Though regularization and renomarlization in computation is still too tough for humans to do).</p><h2 id="Effective-Polynomial-of-Hamiltonian-in-ML"><a href="#Effective-Polynomial-of-Hamiltonian-in-ML" class="headerlink" title="Effective Polynomial of Hamiltonian in ML"></a>Effective Polynomial of Hamiltonian in ML</h2><p>Although the procedure constructing Hamiltonian showing above leave an amazing impression in simplification, one may first doubt the implicit hypothesis that such polynomial effective Hamiltonian in physics is <strong>complete</strong> in ML, i.e., any wanted function can be accurately evaluated by a sufficient large network. In fact, by the celebrated <strong>Stone-Weierstrass thoerem</strong> in real analysis going that polynomial functions can approximate continuous functions in arbitrary accuracy, DNN, denoted by<br>$$\begin{equation}\label{3.7}<br>    f(\bm{x})=\bm{\sigma}_n\bm{A}\cdots\bm{\sigma}_2\bm{A}_2\bm{\sigma}_1\bm{A}_1\bm{x}<br>\end{equation}$$<br>in our notation, where $\bm{\sigma}_i$ indicate the non-linear activation function such as sigmoid, Relu, or softmax functions, and $\bm{A}_i$ indicates affine transformation that $\bm{A}_i\bm{x}\equiv\bm{W}_i\bm{x}+\bm{b}_i$, should also does, provided that the polynomial structure still holds under the action of the neuron network $f$. In fact, a simple neural network of the form $f=\bm{\sigma}_2\bm{\sigma}\bm{A}_1$ with input layer, hidden layer and output layer size 2,4 and 1, exactly satisfies this condition<sup id="fnref:12"><a href="#fn:12" rel="footnote">12</a></sup>.</p><p>Therefore, in ML we can also merely focus on the polynomial series (at a specific layer $\lambda$)<br>$$\begin{align}\label{3.8}<br>H_{y}(\bm{x};\lambda)&amp;=h(\lambda)+\sum_ih_{\lambda i}x_i+\sum_{i&lt;j}h_{ij}(\lambda )x_ix_j\nonumber\\<br>&amp;+\sum_{i&lt;j&lt;k}h_{ijk}(\lambda)x_ix_jx_k+\cdots,<br>\end{align}$$<br>where $x_i$ are the $i$th component of the vector $\bm{x}$ at layer $\lambda$ (not have to be the same size as the input layer), and correlation between layers $\{h,h_i,h_{ij},\cdots\}$ are also labeled by the coarse grained steps $\lambda$.</p><p>The similar form of Hamiltonian in ML is a reminiscent of that in Landau-Ginzberg theory. As is seen before, translation and rotation symmetry of the physical system is certainly not powerful in simplifying Hamiltonian as locality does, which truncates at finite terms and leaves with effective ones, regarding the other terms as perturbation. But we can still find similar operation in ML immitating the translation behaviors-convolution of grayscale pixels, or CNN. And interesting and strange enough, there hitherto seems to exist no appropriate algorithm about extracting rotational symmetry in pattern recognition.</p><p>What really counts in simplifying DNN, in physics perspective, is condition of locality. In physics we attrbute the quadratic form of fundament equations deirved from variational derivatives of action to the finite correlation of separate parts of the system, where quadratic Hamiltonian is enough to grasp the free thoery. For example, the Maxwell equation governing the electromagnetism, the Navier-Stokes equations governing fluid dynamics, and even the non-linear Einstein’s equation of gravitational fields are all quadratic. Another explanation of the emergence of low order effective Hamiltonian central limit theorem, which dominate large number of probability distribution in ML. The commonality in fundamental equations of theoretical physics listed here does not indicate the same behavier on DNN, since we’ve not found an appropriate way to define the ``measure’’ of neurons in the same layer, but this does enlighten us a new angle to understand the training results of neuron network.</p><p>After training of a huge neuron network, one may find some correlation between neurons, namely, tensor coefficients in Hamiltonian $h_{ijk\cdots}$, look extremely <strong>sparse</strong> that has little contribution on evalution of output, and hence can be dropped out from the tumid network with little influence on the new slender one. In this sense, we can safely concludes that <strong>locality corresponds to sparsity</strong> in ML.<br>A typical and non-trivial example is deep RBM we talked before, in which the joint energy (Hamiltonian) between every two visible neurons and hidden neurons is defined to be binanry form $\eqref{2.1}$. Though the structure of RBM looks disparate from the general DNN we write before in $\eqref{3.7}$, if we treat the visible-hidden two layers pairs as in the same ``layer’’ (in the sense I introduced in $\eqref{3.7}$), i.e., they are now sub-layers of one specific layer, deep RBM still embody the spirit of effective theory with the Hamiltonian<br>$$H_\lambda=\int\dd\bm{x}~\bigg[\bigg(K_1(\lambda)\psi+M_1(\lambda)\nabla\psi\bigg)+N_1(\lambda)\psi\nabla\psi\bigg]$$<br>if we apply the correspondence $h_i\leftrightarrow\psi$ and $v_i\leftrightarrow\nabla\psi$. One can see that this is the lowest expanding of Hamiltonian with interactions $N_1(\lambda)\psi\nabla psi$. The reason why we drop the other two two-order free parts $K_2(\lambda)\psi^2$ and $M_2(\lambda)(\nabla\psi)^2$ is that we are now considering the RBM, rather than general BM.</p><p>Motivated by the common procedures of counting Feynman diagrams in quantum fields theory, one should naturally attempt to include higher order terms unless we reach the optimization order of <strong>asymptotic series</strong><sup id="fnref:11"><a href="#fn:11" rel="footnote">11</a></sup> :</p><p>$$H=H^{(1)}_\text{free}+H^{(2)}_{\text{int.}}+H^{(3)}_{\text{high order}}+\cdots,$$</p><p>where $H^{(3)}$ includes self-coupling terms $K_3\psi^3$, $M_3(\nabla\psi)^3$ and high order interactions $N_{3,1}\psi^2\nabla\psi$, $N_{3,1}\psi(\nabla\psi)^2$. But at present the planar diagram in ML to visualize the DNN never works and we have to carefully count the coupling between neurons at the same layer (in the new sense) in the new notation used in <strong>cluster expansion</strong> introduced by Mayer<sup id="fnref:8"><a href="#fn:8" rel="footnote">8</a></sup>.</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://wx4.sinaimg.cn/large/0060CjOygy1flnq45ixpzj30j004w3yq.jpg" alt="**The Third Order Couplings between Neurons**. Unlike the notation widely used in ML that lines between neurons denotes the affine transformation or any other mathematical operations, here lines in diagrams just represent the couplings without any mathematical meaning. Black circles represent the hidden sub-layers while white ones represent the visible sub-layers." title="">                </div>                <div class="image-caption">**The Third Order Couplings between Neurons**. Unlike the notation widely used in ML that lines between neurons denotes the affine transformation or any other mathematical operations, here lines in diagrams just represent the couplings without any mathematical meaning. Black circles represent the hidden sub-layers while white ones represent the visible sub-layers.</div>            </figure><p>Taking the step in consideration of high-order neuron networks, we are now reaching an entirely new world that experts in ML seems to ignore. Hopefully adding an extra interaction among neuron in a layer may improve much more than adding layers behind the NN in practical training, since by our discussion before, adding layers just helps reaching close to the fixed point under actions of RG, but the real fixed points will differ from that described by the low order effective Hamiltonian.</p><h1 id="Conclusion-and-Perspectives"><a href="#Conclusion-and-Perspectives" class="headerlink" title="Conclusion and Perspectives"></a>Conclusion and Perspectives</h1><p>However, even though locality, or sparcity widely exists in physics and ML, people still discover some important exceptions in recent year. In fractional quantum hall effect, a strong-correlated system in condensed matter physics, for example, repulsive interaction between electrons are not screened as it usually does in metal but play a central role in the emergence of a series of strange phenomenon such as fractional charge and boundary topological state instead. In this case, our expanding of Hamiltonian by orders is proved to be invalid because of the coupling constant is now large enough to make the series divergence. Wen<sup id="fnref:10"><a href="#fn:10" rel="footnote">10</a></sup> divided the theory in condensed matter physics into two parts: classic ones are those that can be well explained by Ginzberg-Landau theory, the other modern ones has not be well-studies up to now so there is no unified theory describing them. Despite the large amounts of DNN we discussed in this paper that can be well-understanded and even upgraded by Landau-Ginzberg theory, we believe that there still exits some structure of NN that corrsponds to the modern object in condensed matter physics such as topological states and topological orders, and indeed many groups<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup> are working on the quantum algorithm basing on these strongly-correlated modern concepts.</p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;">@book{haykin2009neural,<br>title={Neural networks and learning machines},<br>author={Haykin, Simon S},<br>volume={3},<br>year={2009},<br>publisher={Pearson Upper Saddle River, NJ, USA:}<br>}</span><a href="#fnref:1" rev="footnote"> ↩</a></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">2.</span><span style="display: inline-block; vertical-align: top;">@book{peskin1995introduction,<br>title={An introduction to quantum field theory},<br>author={Peskin, Michael Edward},<br>year={1995},<br>publisher={Westview press}<br>}</span><a href="#fnref:2" rev="footnote"> ↩</a></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">3.</span><span style="display: inline-block; vertical-align: top;">@article{Wilson-Renormalization&amp;Critical,<br>title={The renormalization group and critical phenomena},<br>author={Wilson, Kenneth G},<br>journal={Reviews of Modern Physics},<br>volume={55},<br>number={3},<br>pages={583},<br>year={1983},<br>publisher={APS}<br>}</span><a href="#fnref:3" rev="footnote"> ↩</a></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">4.</span><span style="display: inline-block; vertical-align: top;">@book{goodfellow2016deep,<br>title={Deep learning},<br>author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},<br>year={2016},<br>publisher={MIT press}<br>}</span><a href="#fnref:4" rev="footnote"> ↩</a></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">5.</span><span style="display: inline-block; vertical-align: top;">@article{carrasquilla2017machine,<br>title={Machine learning phases of matter},<br>author={Carrasquilla, Juan and Melko, Roger G},<br>journal={Nature Physics},<br>year={2017},<br>publisher={Nature Research}<br>}</span><a href="#fnref:5" rev="footnote"> ↩</a></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">6.</span><span style="display: inline-block; vertical-align: top;">@article{top,<br>title={Exact machine learning topological states},<br>author={Deng, Dong-Ling and Li, Xiaopeng and Sarma, S Das},<br>journal={arXiv preprint arXiv:1609.09060},<br>year={2016}<br>}</span><a href="#fnref:6" rev="footnote"> ↩</a></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">7.</span><span style="display: inline-block; vertical-align: top;">@book{Francesco-CFT,<br>title={Conformal field theory},<br>author={Francesco, Philippe and Mathieu, Pierre and S{‘e}n{‘e}chal, David},<br>year={2012},<br>publisher={Springer Science &amp; Business Media}<br>}</span><a href="#fnref:7" rev="footnote"> ↩</a></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">8.</span><span style="display: inline-block; vertical-align: top;">@book{Kardar-fields,<br>title={Statistical physics of fields},<br>author={Kardar, Mehran},<br>year={2007},<br>publisher={Cambridge University Press}<br>}</span><a href="#fnref:8" rev="footnote"> ↩</a></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">9.</span><span style="display: inline-block; vertical-align: top;">@book{phillips2012advanced,<br>title={Advanced solid state physics},<br>author={Phillips, Philip},<br>year={2012},<br>publisher={Cambridge University Press}<br>}</span><a href="#fnref:9" rev="footnote"> ↩</a></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">10.</span><span style="display: inline-block; vertical-align: top;">@book{Wen-QFT,<br>title={Quantum field theory of many-body systems: from the origin of sound to an origin of light and electrons},<br>author={Wen, Xiao-Gang},<br>year={2004},<br>publisher={Oxford University Press on Demand}<br>}</span><a href="#fnref:10" rev="footnote"> ↩</a></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">11.</span><span style="display: inline-block; vertical-align: top;">@book{nakahara2003geometry,<br>title={Geometry, topology and physics},<br>author={Nakahara, Mikio},<br>year={2003},<br>publisher={CRC Press}<br>}</span><a href="#fnref:11" rev="footnote"> ↩</a></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">13.</span><span style="display: inline-block; vertical-align: top;">@article{why-deep&amp;cheap,<br>title={Why does deep and cheap learning work so well?},<br>author={Lin, Henry W and Tegmark, Max and Rolnick, David},<br>journal={Journal of Statistical Physics},<br>volume={168},<br>number={6},<br>pages={1223–1247},<br>year={2017},<br>publisher={Springer}<br>}</span><a href="#fnref:13" rev="footnote"> ↩</a></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Deep neural network (DNN) works in an amazing effeciency in machine learing but cannot be well-explained by accurate mathematics. Reviewing the one-to-one correspondence between Restricted Boltzmann machines (RBM) and configuration space renomalization groups (RG), the well-developed techniques in studying quantum fields theory (QFT), we are motivated to apply the effective fields theory, particularly Landau-Ginzberg one, to explain the expressibility and mechnism of a general DNN. Similar structures of the effective Hamiltionian are discovered, and we propose a new model based on the systematic cluster expansion approach to improve the performance of DNN without adding extra layer. Supported by the high efficiency in counting low orders Feynman diagrams in computation of scattering amplitute by QFT, the upgrade of DNN we propose is believed to provide a higher efficiency than extending the depth of DNN if the coarse graining procedure has neared the fixed point of RG flows. &lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
      <category term="Course Paper" scheme="http://yoursite.com/categories/Machine-Learning/Course-Paper/"/>
    
    
      <category term="Machine Learing" scheme="http://yoursite.com/tags/Machine-Learing/"/>
    
      <category term="Ginzberg-Landau Theory" scheme="http://yoursite.com/tags/Ginzberg-Landau-Theory/"/>
    
      <category term="Cluster Expansion" scheme="http://yoursite.com/tags/Cluster-Expansion/"/>
    
      <category term="Renormalization Group" scheme="http://yoursite.com/tags/Renormalization-Group/"/>
    
      <category term="Boltzmann Machine" scheme="http://yoursite.com/tags/Boltzmann-Machine/"/>
    
  </entry>
  
  <entry>
    <title>Chapter 2: Group Categories</title>
    <link href="http://yoursite.com/2017/05/14/Chapter-2-Group-Categories/"/>
    <id>http://yoursite.com/2017/05/14/Chapter-2-Group-Categories/</id>
    <published>2017-05-14T01:07:48.000Z</published>
    <updated>2017-11-23T01:24:18.052Z</updated>
    
    <content type="html"><![CDATA[<p>Chapter 2 studies group categories. We introduce the concepts of free and underlying functors with examples of free group without giving the explicit definition.<br><a id="more"></a></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-20.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-21.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-22.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-23.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-24.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-25.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-26.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-27.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-28.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-29.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-30.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-31.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Chapter 2 studies group categories. We introduce the concepts of free and underlying functors with examples of free group without giving the explicit definition.&lt;br&gt;
    
    </summary>
    
      <category term="Mathematical Physics" scheme="http://yoursite.com/categories/Mathematical-Physics/"/>
    
      <category term="理论格物论" scheme="http://yoursite.com/categories/Mathematical-Physics/%E7%90%86%E8%AE%BA%E6%A0%BC%E7%89%A9%E8%AE%BA/"/>
    
      <category term="Category Theory" scheme="http://yoursite.com/categories/Mathematical-Physics/%E7%90%86%E8%AE%BA%E6%A0%BC%E7%89%A9%E8%AE%BA/Category-Theory/"/>
    
    
      <category term="Group Category" scheme="http://yoursite.com/tags/Group-Category/"/>
    
      <category term="Free Group" scheme="http://yoursite.com/tags/Free-Group/"/>
    
      <category term="Free Functos" scheme="http://yoursite.com/tags/Free-Functos/"/>
    
      <category term="Underlying Functors" scheme="http://yoursite.com/tags/Underlying-Functors/"/>
    
  </entry>
  
  <entry>
    <title>Chapter 1: Catergories and Functors</title>
    <link href="http://yoursite.com/2017/04/16/Chapter-1-Catergories-and-Functors/"/>
    <id>http://yoursite.com/2017/04/16/Chapter-1-Catergories-and-Functors/</id>
    <published>2017-04-16T00:40:54.000Z</published>
    <updated>2017-11-23T00:48:11.287Z</updated>
    
    <content type="html"><![CDATA[<p>Chapter 1 starts from cateogories and their products and coproducts, then discuss functors and their natural transformations, as well as adjoint ones.<br><a id="more"></a></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-8.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-9.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-10.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-11.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-12.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-13.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-14.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-15.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-16.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-17.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-18.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-19.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Chapter 1 starts from cateogories and their products and coproducts, then discuss functors and their natural transformations, as well as adjoint ones.&lt;br&gt;
    
    </summary>
    
      <category term="Mathematical Physics" scheme="http://yoursite.com/categories/Mathematical-Physics/"/>
    
      <category term="理论格物论" scheme="http://yoursite.com/categories/Mathematical-Physics/%E7%90%86%E8%AE%BA%E6%A0%BC%E7%89%A9%E8%AE%BA/"/>
    
      <category term="Category Theory" scheme="http://yoursite.com/categories/Mathematical-Physics/%E7%90%86%E8%AE%BA%E6%A0%BC%E7%89%A9%E8%AE%BA/Category-Theory/"/>
    
    
      <category term="Category" scheme="http://yoursite.com/tags/Category/"/>
    
      <category term="Functor" scheme="http://yoursite.com/tags/Functor/"/>
    
      <category term="Product" scheme="http://yoursite.com/tags/Product/"/>
    
      <category term="Coproduct" scheme="http://yoursite.com/tags/Coproduct/"/>
    
      <category term="Natural Transformation" scheme="http://yoursite.com/tags/Natural-Transformation/"/>
    
      <category term="Adjoint Functors" scheme="http://yoursite.com/tags/Adjoint-Functors/"/>
    
  </entry>
  
  <entry>
    <title>Mathematical Physics: Category Theory</title>
    <link href="http://yoursite.com/2017/03/20/Mathematical-Physics-Category-Theory/"/>
    <id>http://yoursite.com/2017/03/20/Mathematical-Physics-Category-Theory/</id>
    <published>2017-03-20T06:12:05.000Z</published>
    <updated>2017-11-23T00:53:19.180Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-0.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-4.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-6.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ozpcak42w.bkt.clouddn.com/Category-7.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
 
      
    
    </summary>
    
      <category term="Mathematical Physics" scheme="http://yoursite.com/categories/Mathematical-Physics/"/>
    
      <category term="Category Theory" scheme="http://yoursite.com/categories/Mathematical-Physics/Category-Theory/"/>
    
      <category term="理论格物论" scheme="http://yoursite.com/categories/Mathematical-Physics/Category-Theory/%E7%90%86%E8%AE%BA%E6%A0%BC%E7%89%A9%E8%AE%BA/"/>
    
    
      <category term="Preface" scheme="http://yoursite.com/tags/Preface/"/>
    
  </entry>
  
  <entry>
    <title>Mathjax on my Hexo</title>
    <link href="http://yoursite.com/2016/06/24/Mathjax-on-my-Hexo/"/>
    <id>http://yoursite.com/2016/06/24/Mathjax-on-my-Hexo/</id>
    <published>2016-06-23T17:49:54.000Z</published>
    <updated>2017-11-20T15:25:30.048Z</updated>
    
    <content type="html"><![CDATA[<hr><p>Thic page record the configuration of my hexo theme <strong>indigo</strong>.</p><a id="more"></a><h1 id="Configuration-Files"><a href="#Configuration-Files" class="headerlink" title="Configuration Files"></a>Configuration Files</h1><h2 id="Require-Necessary-Packages-and-Define-Macros-for-Convenience-and-Compatibility"><a href="#Require-Necessary-Packages-and-Define-Macros-for-Convenience-and-Compatibility" class="headerlink" title="Require Necessary Packages and Define Macros for Convenience and Compatibility"></a>Require Necessary Packages and Define Macros for Convenience and Compatibility</h2><pre><code>&lt;% if (theme.mathjax){ %&gt;&lt;!-- mathjax config similar to math.stackexchange --&gt;&lt;script type=&quot;text/x-mathjax-config&quot;&gt;MathJax.Hub.Config({    tex2jax: {        inlineMath: [ [&apos;$&apos;,&apos;$&apos;], [&quot;\\(&quot;,&quot;\\)&quot;]  ],    displayMath: [ [&apos;$$&apos;, &apos;$$&apos;] ],        processEscapes: true,        skipTags: [&apos;script&apos;, &apos;noscript&apos;, &apos;style&apos;, &apos;textarea&apos;, &apos;pre&apos;, &apos;code&apos;]    },    TeX: {    equationNumbers: { autoNumber: &quot;AMS&quot; },    extensions: [&quot;AMSmath.js&quot;, &quot;AMSsymbols.js&quot;, &quot;AMScd.js&quot;, &quot;mediawiki-texvc.js&quot;],    Macros: {        bm: &quot;\\boldsymbol&quot;,        dd: &quot;\\mathop{}\\!\\mathrm{d}&quot;,    }    }});</code></pre><h2 id="Add-Numbers-for-Equations"><a href="#Add-Numbers-for-Equations" class="headerlink" title="Add Numbers for Equations"></a>Add Numbers for Equations</h2><pre><code>MathJax.Hub.Queue(function() {    var all = MathJax.Hub.getAllJax(), i;    for(i=0; i &lt; all.length; i += 1) {        all[i].SourceElement().parentNode.className += &apos; has-jax&apos;;    }});&lt;/script&gt;</code></pre><h2 id="Accelate-Analysis"><a href="#Accelate-Analysis" class="headerlink" title="Accelate Analysis:"></a>Accelate Analysis:</h2><pre><code>&lt;script async src=&quot;//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;&lt;% } %&gt;</code></pre><h1 id="Mathjax-Tests"><a href="#Mathjax-Tests" class="headerlink" title="Mathjax Tests"></a>Mathjax Tests</h1><h2 id="AMS-Font-Test"><a href="#AMS-Font-Test" class="headerlink" title="AMS Font Test"></a>AMS Font Test</h2><pre><code>$$\mathcal{ABC}\mathbf{ABC}\mathfrak{ABC}\mathscr{ABC}\mathsf{abc}.$$</code></pre><p>$$\mathcal{ABC}\mathbf{ABC}\mathfrak{ABC}\mathscr{ABC}\mathsf{abc}.$$</p><h2 id="Matrix-Test"><a href="#Matrix-Test" class="headerlink" title="Matrix Test"></a>Matrix Test</h2><pre><code>$$\chi(\lambda) = \left| \begin{array}{ccc}\lambda - a &amp; -b &amp; -c \\-d &amp; \lambda - e &amp; -f \\-g &amp; -h &amp; \lambda - i\end{array} \right|.$$</code></pre><p>$$<br>\chi(\lambda) = \left| \begin{array}{ccc}<br>\lambda - a &amp; -b &amp; -c \\<br>-d &amp; \lambda - e &amp; -f \\<br>-g &amp; -h &amp; \lambda - i<br>\end{array} \right|.<br>$$</p><h2 id="Commutative-Diagrams-Test"><a href="#Commutative-Diagrams-Test" class="headerlink" title="Commutative Diagrams Test"></a>Commutative Diagrams Test</h2><pre><code>$$\begin{CD}A @&lt;&lt;&lt; B @&gt;&gt;&gt; C \\  @.     @|   @AAA \\  @.   D @=   E.\end{CD}$$</code></pre><p>$$\begin{CD}<br>A @&lt;&lt;&lt; B @&gt;&gt;&gt; C \\<br>  @.     @|   @AAA \\<br>  @.   D @=   E.<br>\end{CD}$$</p>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;Thic page record the configuration of my hexo theme &lt;strong&gt;indigo&lt;/strong&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="Mathjax" scheme="http://yoursite.com/categories/Mathjax/"/>
    
    
      <category term="Mathjax" scheme="http://yoursite.com/tags/Mathjax/"/>
    
      <category term="LaTeX" scheme="http://yoursite.com/tags/LaTeX/"/>
    
      <category term="Javascript" scheme="http://yoursite.com/tags/Javascript/"/>
    
  </entry>
  
  <entry>
    <title>Derivation of Einstein&#39;s Fields Equation</title>
    <link href="http://yoursite.com/2016/04/08/Einstein%20Equation-Variation/"/>
    <id>http://yoursite.com/2016/04/08/Einstein Equation-Variation/</id>
    <published>2016-04-08T10:28:05.000Z</published>
    <updated>2017-11-19T12:16:19.991Z</updated>
    
    <content type="html"><![CDATA[<p>We are going to cover two ways of the derivation of Einstein’s equation, one is based on physical analysis, anther one utilizing functional derivatives.</p><h2 id=""><a href="#" class="headerlink" title=""></a><a id="more"></a></h2><h1 id="Energy-Momentum-Tensor"><a href="#Energy-Momentum-Tensor" class="headerlink" title="Energy-Momentum Tensor"></a>Energy-Momentum Tensor</h1><hr><h2 id="Def-Energy-Momentum-Tensor"><a href="#Def-Energy-Momentum-Tensor" class="headerlink" title="Def(Energy-Momentum Tensor):"></a>Def(Energy-Momentum Tensor):</h2><p>Given the action $I$, the energy momentum tensor of a physical system is defined as</p><p>$$T_{\mu\nu}:=-\dfrac{2}{\sqrt{-g}}\dfrac{\delta I}{\delta g^{\mu\nu}},$$</p><p>where $\displaystyle\dfrac{\delta I}{\delta g^{\mu\nu}}$ is variational derivatives and $g$ represents the determinant of matrix $(g_{\mu\nu})$.</p><hr><h2 id="Example"><a href="#Example" class="headerlink" title="Example:"></a>Example:</h2><p>In electrodynamics, the action is (there may be some typos about the coefficients, but that does not matter at all)<br>$$<br>\begin{equation}\label{1.1.1}I_{em}=-\dfrac{1}{4}\int d^{4}x\sqrt{-g}F^{\mu\nu}F_{\mu\nu}.\end{equation}<br>$$</p><p>If we introduce $\text{magnetic vector protential }A_{\mu}$ (It is a valuable to check that this definition automatically satisfies the 4-dimensional Maxell’s equation $\partial_{[\alpha}F_{\mu\nu]}=0$. Additionally, you can re-express the Maxell’s equation in a more symmetric form by $\text{Hodge star }*:\Lambda^{k}(V)\rightarrow\Lambda^{n-k}(V)$ in differential manifolds. Here I just skip it because it thouroughly digresses from the topic :) )</p><p>$$F_{\mu\nu}\equiv\partial_{\mu}A_{\nu}-\partial_{\nu}A_{\mu}.$$</p><p>Then one can show that the energy momentum defined here is compatible with that we already knew in electrodynamics classes:</p><hr><h2 id="Claim-Energy-Momentum-Tensor-of-Electrodynamics"><a href="#Claim-Energy-Momentum-Tensor-of-Electrodynamics" class="headerlink" title="Claim(Energy-Momentum Tensor of Electrodynamics):"></a>Claim(Energy-Momentum Tensor of Electrodynamics):</h2><p>$$<br>\begin{equation}\label{1.1.2}T_{\rho\sigma}^{em}=-\dfrac{2}{\sqrt{-g}}\dfrac{\delta I_{em}}{\delta g^{\rho\sigma}}=-\dfrac{1}{4}g_{\rho\sigma}F^{2}+F_{\rho\nu}F_{\sigma}^{~\nu}.\end{equation}<br>$$</p><hr><h2 id="Proof"><a href="#Proof" class="headerlink" title="Proof:"></a>Proof:</h2><p>(skipped)</p><hr><h1 id="Derivation-Based-on-Functional-Derivatives"><a href="#Derivation-Based-on-Functional-Derivatives" class="headerlink" title="Derivation Based on Functional Derivatives"></a>Derivation Based on Functional Derivatives</h1><hr><p>Undoubtedly it is based on talent analyses on physics that Einstein firstly write down the Field Equation. The reason why I give you another approach at first is because of some pedagogical consideration.</p><hr><h2 id="Axiom-Hilbert-Action"><a href="#Axiom-Hilbert-Action" class="headerlink" title="Axiom:(Hilbert Action)"></a>Axiom:(Hilbert Action)</h2><p>The action of general relativity is given by Hilbert that</p><p>$$I_{G}=\dfrac{1}{16\pi G}\int dx^{4}\sqrt{-g}R,$$</p><p>where $R$ is Ricci scalar.</p><hr><h2 id="Proposition-Einstein’s-Field-Equation"><a href="#Proposition-Einstein’s-Field-Equation" class="headerlink" title="Proposition(Einstein’s Field Equation):"></a>Proposition(Einstein’s Field Equation):</h2><p>$$<br>\begin{equation}\label{1.2.1}R_{\mu\nu}-\dfrac{1}{2}g_{\mu\nu}R=8\pi GT_{\mu\nu}\end{equation}<br>$$</p><hr><h2 id="Proof-1"><a href="#Proof-1" class="headerlink" title="Proof:"></a>Proof:</h2><p>$$\delta I_{G}=\dfrac{1}{16\pi G}\int dx^{4}\bigg[(\delta\sqrt{-g})g^{\mu\nu}R_{\mu\nu}+\sqrt{-g}(\delta g^{\mu\nu})R_{\mu\nu}+\sqrt{-g}g^{\mu\nu}\delta R_{\mu\nu}\bigg].$$</p><p>There is no harm to consider the derivation in the local inertial coordinate(<strong>think why?</strong>), that is, $g_{\mu\nu}\rightarrow\eta_{\mu\nu}$ and $\varGamma_{\mu\nu}^{\sigma}=0$. Thus,<br>$$\delta R_{\mu k}=\partial_{k}\delta\varGamma_{\mu\lambda}^{\lambda}-\partial_{\lambda}\delta\varGamma_{\mu k}^{\lambda}+0+0-0-0.$$</p><p>And it is easy to check that $\delta\varGamma$ is always a tensor<span style="color: #000000;">.</span><strong><span style="color: #000000;">(verify!) <span style="color: #333333;">(</span></span><span style="color: #333333;">Hint:</span> Consider two distinct connections defined on the manifold and their affine connection’s behaviors under the coordinate tansformation.)</strong></p><p>In this way,<br>$$<br>\begin{align}\sqrt{-g}g^{\mu\nu}R_{\mu\nu}&amp;=\sqrt{-g}g^{\mu\nu}(\partial_{\nu}\delta\varGamma_{\mu\lambda}^{\lambda})-\sqrt{-g}g^{\mu\nu}(\partial_{\lambda}\delta\varGamma_{\mu\nu}^{\lambda})\nonumber\\=&amp;\partial_{\nu}(\sqrt{-g}g^{\mu\nu}\delta\varGamma_{\mu\lambda}^{\lambda})-\partial_{\lambda}(\sqrt{-g}g^{\mu\nu}\delta\varGamma_{\mu\nu}^{\lambda}),\end{align}<br>$$<br>which vanishes at the boundary of spacetime by integrating by parts.</p><p>So<br>$$<br>\begin{align}\delta I_{G}&amp;=\dfrac{1}{16\pi G}\int dx^{4}\bigg[(\delta\sqrt{-g})g^{\mu\nu}R_{\mu\nu}+\sqrt{-g}(\delta g^{\mu\nu})R_{\mu\nu}\bigg]\nonumber\\&amp;=\dfrac{1}{16\pi G}\int dx^{4}\left[\dfrac{\sqrt{-g}}{2}g^{\rho\sigma}g^{\mu\nu}R_{\mu\nu}\delta g_{\rho\sigma}+\sqrt{-g}(-g^{\mu\rho}g^{\nu\sigma}\delta g_{\rho\sigma})R_{\mu\nu}\right]\nonumber\\&amp;=\dfrac{1}{16\pi G}\int dx^{4}\sqrt{-g}\delta g_{\rho\sigma}\left(\dfrac{1}{2}g^{\rho\sigma}R-R^{\rho\sigma}\right),\nonumber\end{align}<br>$$<br>or (<strong>there are some nontivial indice contraction tricks in the above computation that I believe you can handle, so I just left them as exercises.</strong>)</p><p>$$\displaystyle\dfrac{\delta I_{G}}{\delta g^{\rho\sigma}}=\dfrac{\sqrt{-g}}{16\pi G}\left(-\dfrac{1}{2}g^{\rho\sigma}R-R^{\rho\sigma}\right).$$</p><p>On the other aspect, $\displaystyle\dfrac{\delta I_{G}}{\delta g^{\rho\sigma}}=-\dfrac{\sqrt{-g}}{2}T^{\rho\sigma}$.</p><p>At last, by reducing the indices we get $\text{Einstein’s Field Equation}$:</p><p>$$<br>\begin{equation}<br>R_{\mu\nu}-\dfrac{1}{2}g_{\mu\nu}R=8\pi GT_{\mu\nu}.<br>\end{equation}<br>$$</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;We are going to cover two ways of the derivation of Einstein’s equation, one is based on physical analysis, anther one utilizing functional derivatives.&lt;/p&gt;
&lt;h2 id=&quot;&quot;&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title=&quot;&quot;&gt;&lt;/a&gt;
    
    </summary>
    
      <category term="General Relativity" scheme="http://yoursite.com/categories/General-Relativity/"/>
    
    
      <category term="Newtonian Limits" scheme="http://yoursite.com/tags/Newtonian-Limits/"/>
    
      <category term="Variational Methods" scheme="http://yoursite.com/tags/Variational-Methods/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Homology Group</title>
    <link href="http://yoursite.com/2016/04/04/Introduction-to-Homology-Group/"/>
    <id>http://yoursite.com/2016/04/04/Introduction-to-Homology-Group/</id>
    <published>2016-04-04T12:26:35.000Z</published>
    <updated>2017-11-20T15:21:41.061Z</updated>
    
    <content type="html"><![CDATA[<p>To have a quick view of homology, I’ll use non-formal language to introduce homology group and then take $T^{2}$ as an simple example to illustrate the motivation of its definition and construction.</p><p>Let’s take a look at $S^{2}$ and $T^{2}$.</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://wx3.sinaimg.cn/large/0060CjOygy1flnp4gxwfzj30jm0g73zm.jpg" alt="Triangulation of $S^2$" title="">                </div>                <div class="image-caption">Triangulation of $S^2$</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://wx1.sinaimg.cn/large/0060CjOygy1flnp4iu154j30jm0g7jsi.jpg" alt="Triangulation of $T^2$" title="">                </div>                <div class="image-caption">Triangulation of $T^2$</div>            </figure><p>It’s obvious that the topological structure of these two manifolds are essentially different. On the surface, $S^{2}$ has no hole while $T^{2}$ has one. However, we can mathematically reinterpret this difference as that any circle $S^{1}$ on the $S^{2}$ encloses a domain, that is, $\forall S^{1}\subset S^{2},\exists \text{close disk }B\subset S^{2} \text{ s.t. } S^{1}=\partial B$. On the contrary, there are two kinds of circle on $T^{2}$ that do not enclose any domain: latitude and longitude circle.(One incision but two boundaries)</p><p>Through closely observation one can find that although one circle(either latitude or longitude one) cannot enclose a domain, two circle of the same type can do. What does this mean?</p><blockquote><p>Two longitude(or latitude) circle has no difference on reflecting topological structure of $T^{2}$, in other words, they belongs to the same class. Moreover, any longitude circle and latitude circle cannot enclose a domain, so they belongs to two different class.</p></blockquote><p>In mathematics, this compact(closed) class of sub-manifolds which does not enclose a domain and can be orientated are called ??? ( 英语不会。。可定向无边紧子流形类, there is no harm that I personally call this as OSCNB). They are exactly the generators of homology groups.</p><p>Take $T^{2}$ for example, zero-dimension OSCNB is just the class $[p]$ that is generated by one point $p\in T^{2}$. Because any distinct point can enclose a segment on $T^{2}$, there are two kinds of one-dimensional OSCNB, as is discussed before(latitude and longitude ones). Additionally, $T^{2}$ itself is a two-dimensional OSCNB.</p><p>In this way, k-dimensional homology group of  $T^{2}$, denoted as $H_{k}(T^{2},\mathbb{R})$, is a group showing as follow:</p><p>$$\begin{align}<br>H_{0}(T^{2},\mathbb{R})&amp;=\{\alpha e_{0}|\alpha\in\mathbb{R}, e_{0}=[p]\}\cong\mathbb{R},\nonumber\\<br>H_{1}(T^{2},\mathbb{R})&amp;=\{\alpha_{1}e_{1}+\alpha_{2}e_{2}|\alpha_{i}\in\mathbb{R}, e_{i}=[\sigma_{i}], i=1,2\}\cong\mathbb{R}^{2},\nonumber\\<br>H_{2}(T^{2},\mathbb{R})&amp;=\{\alpha e_{2}|\alpha\in\mathbb{R}, e_{2}=[T^{2}]\}\cong\mathbb{R},\nonumber<br>\end{align}$$</p><p>Here, $[\sigma_{1}]$ denotes class of longitude circles, while $[\sigma_{2}]$ denotes class of latitude circles.</p><p>In the same way, k-dimensional homology group of $S^{2}$ is</p><p>$$\begin{align}<br>H_{0}(S^{2},\mathbb{R})&amp;=\{\alpha e_{0}|\alpha\in\mathbb{R}, e_{0}=[p]\}\cong\mathbb{R},\nonumber\\<br>H_{1}(S^{2},\mathbb{R})&amp;=0, \nonumber \\<br>H_{2}(S^{2},\mathbb{R})&amp;=\{\alpha e_{2}|\alpha\in\mathbb{R}, e_{2}=[S^{2}]\}\cong\mathbb{R}.\nonumber<br>\end{align}$$</p><p>In fact, for general n-dimensional compact manifolds $M$, its homology group is that which takes its OSCNB as generators and a commutative addition group $G$ as its coefficients. For instance, if $G=\mathbb{R}$, all k-dimensional homology group with real coefficients are</p><p>$$\begin{align}<br>H_{k}(M,\mathbb{R})&amp;=\left\{\left.\sum_{j=1}^{m_{k}}\alpha_{j}e_{j}^{k}\right|\alpha_{j}\in\mathbb{R}, 1\leqslant j\leqslant m_{k}\right\}\cong\mathbb{R},\quad 0\leqslant k\leqslant n,\nonumber\\<br>H_{k}(M,\mathbb{R})&amp;=0,\quad\forall k&gt;n\nonumber<br>\end{align}$$</p><p>The original purpose of constructing Homology group is to find topological invariants in topological space(to prove this we should show that homology groups defined here is invariant under partition, and that’s another topic), that is, two homeomorphism topological space must have isomorphism homology group. </p><p>At this point can we safely conclude that $S^{2}$ and $T^{2}$ are topological distinct because their one-dimensional homology groups are different !</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;To have a quick view of homology, I’ll use non-formal language to introduce homology group and then take $T^{2}$ as an simple example to 
      
    
    </summary>
    
      <category term="Mathematical Physics" scheme="http://yoursite.com/categories/Mathematical-Physics/"/>
    
    
      <category term="Homology Goup" scheme="http://yoursite.com/tags/Homology-Goup/"/>
    
      <category term="Mathematica" scheme="http://yoursite.com/tags/Mathematica/"/>
    
  </entry>
  
  <entry>
    <title>临江仙·加冠</title>
    <link href="http://yoursite.com/2016/03/25/%E4%B8%B4%E6%B1%9F%E4%BB%99%C2%B7%E5%8A%A0%E5%86%A0/"/>
    <id>http://yoursite.com/2016/03/25/临江仙·加冠/</id>
    <published>2016-03-25T12:52:40.000Z</published>
    <updated>2017-11-20T03:23:59.241Z</updated>
    
    <content type="html"><![CDATA[<p>《礼记•曲礼上》“男子二十，冠而字”。三月廿四，仆填词以自述。</p><a id="more"></a><hr><p><strong>弱冠表德</strong>[1]<strong>赐剑履</strong>[2]<strong>，也羡千里豪行</strong>[3]<strong>。穷哭</strong>[3]<strong>广武楚魂听</strong>[5]<strong>。东军</strong>[6]<strong>陷漠北</strong>[7]<strong>，传世飞将</strong>[8]<strong>名。</strong></p><p><strong>吟哦三余</strong>[9]<strong>犯雕辇</strong>[10]<strong>，才得推敲妙语</strong>[11]<strong>。未妨柯烂</strong>[12]<strong>斧还新。授弈仙子</strong>[13]<strong>路</strong>[14]<strong>，负手</strong>[14]<strong>制纹枰</strong>[15]。</p><hr><p>【注】</p><p>[1]弱冠：《礼记 曲礼上》：「男子二十，冠而字」；表德：《颜氏家训 风操》「古者，名以正体，字以表德」<br>[2]剑履：语出《三国志 曹真传》「四年，朝洛阳，迁大司马，赐剑履上殿，入超不趋」<br>[3]千里豪行：李白《侠客行》「十步杀一人，千里不留行」。此处所指，虽今日加冠佩剑，亦不忘太白游侠之想。<br>[4]穷哭：《晋书 卷四九》（阮籍）「时率意独驾，不由径路，车迹所穷，辙恸哭而反」<br>[5]广武楚魂：同《晋书 卷四九》（阮籍）「尝登广武山，观楚汉战处，叹曰‘时无英雄，使竖子成名’」<br>[6]东军：指李广部众。《史记 李广列传》「广不谢大将军而起行，意甚愠而就部，引兵与右将军食其合军而出东道」<br>[7]陷漠北：漠北之战，李广怒出东道，军亡导，或失道，而后大将军卫青。后自愧难对刀笔吏而自刎。<br>[8]飞将：李广。<br>[9]三余：本出自《三国志 魏书 钟繇华歆王朗传》「冬者，岁之余也；夜者，日之余也；阴雨者，时之余也」，意味好学，本处形容吟哦废寝忘食之态。亦可简单作虚词解。<br>[10]推敲妙语：贾岛炼字忘神，倒骑驴而犯韩愈车辇，得「僧敲月下门」之佳句。<br>[11]柯：斧柄。<br>[12]受益仙子：任昉《述异记》「晋时王质伐木至，见童子数人棋而歌，质因听之。童子以一物与质，如枣核，质含之而不觉饥。俄顷，童子谓曰：‘何不去？’质起视，斧柯尽烂，既归，无复时人」，古多用于慨叹恍若隔世感，此处用其观棋入定忘神意。<br>[13]路：棋道。<br>[14]负手：背着手。指棋艺大成，已有近世吴清源让天下一先之力。</p><p>【释】</p><p>既加冠，宜为大人事，首句却反道而行，先表赤子之心：冠弱未稳，剑法生疏，或得执剑上殿，亦不忘少年游侠痴梦，可谓中二依旧也。而前路未可知，或有穷途难行之时，任如阮嗣宗般哭天泣地，也仅有楚魂来听罢。然则，李广失路漠北，不过几日迟后，而军情不误。飞将之名，无有损益！</p><p>贾岛撞辇推敲，王质观棋授弈，均有所获，盖大道归一，本无所谓功成之正途。凡至极处，皆可大成。曷不不舍而锲之？</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《礼记•曲礼上》“男子二十，冠而字”。三月廿四，仆填词以自述。&lt;/p&gt;
    
    </summary>
    
      <category term="Peom" scheme="http://yoursite.com/categories/Peom/"/>
    
    
      <category term="宋词" scheme="http://yoursite.com/tags/%E5%AE%8B%E8%AF%8D/"/>
    
  </entry>
  
  <entry>
    <title>Unifying the Annoying Notation of Riemann Curvature Tensor</title>
    <link href="http://yoursite.com/2016/03/25/Unifying-the-Annoying-Notation-of-Riemann-Curvature-Tensor/"/>
    <id>http://yoursite.com/2016/03/25/Unifying-the-Annoying-Notation-of-Riemann-Curvature-Tensor/</id>
    <published>2016-03-25T12:20:09.000Z</published>
    <updated>2017-11-19T12:24:47.570Z</updated>
    
    <content type="html"><![CDATA[<p>Having had enough of the different forms of Riemann Curvature Tensor, I finally decided to recommend the “most correct” notation of Riemann Curvature here.</p><a id="more"></a><p>In mathematics books, we say $\text{Operator }R:\Gamma(TM)\times\Gamma(TM)\times\Gamma(TM)\rightarrow\Gamma(TM)$ is $\text{the curvature of connection } D$ (Here connection is a general one which may not be torsion-free), if it satisfies:</p><p>$$R(X,Y)Z=D_{X}D_{Y}Z-D_{Y}D_{X}Z-D_{[X,Y]}Z.$$</p><p>In the definition of $\text{Affine Connection Coefficient}$, we have $D_{e_{i}}e_{j}=\Gamma_{ij}^{k}e_{k}$ for any tangent frame field $e_{1},\cdots,e_{m}$, thus</p><p>\begin{align}<br>R\left(\dfrac{\partial}{\partial x^{i}},\dfrac{\partial}{\partial x^{j}}\right)\dfrac{\partial}{\partial x^{k}}&amp;=:R_{kij}^{l}\dfrac{\partial}{\partial x^{l}}\nonumber\\<br>&amp;=D_{\partial/\partial x^{i}}\left(\Gamma_{jk}^{l}\dfrac{\partial}{\partial x^{l}}\right)-D_{\partial/\partial x^{j}}\left(\Gamma_{ik}^{l}\dfrac{\partial}{\partial x^{l}}\right)\nonumber\\<br>&amp;=\left(\partial_{i}\Gamma_{jk}^{l}-\partial_{j}\Gamma_{ik}^{l}\right)\dfrac{\partial}{\partial x^{l}}+\Gamma_{jk}^{m}\Gamma_{im}^{n}\delta_{n}^{l}\dfrac{\partial}{\partial x^{l}}-\Gamma_{ik}^{m}\Gamma_{jm}^{n}\delta_{n}^{l}\dfrac{\partial}{\partial x^{l}}\nonumber\\<br>&amp;=\left(\partial_{i}\Gamma_{jk}^{l}-\partial_{j}\Gamma_{ik}^{l}+\Gamma_{jk}^{m}\Gamma_{im}^{l}-\Gamma_{ik}^{m}\Gamma_{jm}^{l}\right)\dfrac{\partial}{\partial x^{l}}<br>\end{align}</p><p>Here I list some typically torion-free Reimann Curvature Tensor for readers to compare and memorize:</p><ul><li>Lu(卢建新) &amp; Chen(陈维恒):</li></ul><p>$$R_{\text{(陈.) }kij}^{l}=\partial_{i}\Gamma_{kj}^{i}-\partial_{j}\Gamma_{ki}^{l}+\Gamma_{kj}^{h}\Gamma_{kh}^{l}-\Gamma_{ki}^{h}\Gamma_{kj}^{l}.$$</p><ul><li>S.Weinberg:</li></ul><p>$$R_{\text{(W.) }\mu\nu k}^{\lambda}=-R_{\text{(陈.) }\mu\nu k}^{\lambda}$$</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Having had enough of the different forms of Riemann Curvature Tensor, I finally decided to recommend the “most correct” notation of Riemann Curvature here.&lt;/p&gt;
    
    </summary>
    
      <category term="General Relativity" scheme="http://yoursite.com/categories/General-Relativity/"/>
    
    
      <category term="Riemann Curvature" scheme="http://yoursite.com/tags/Riemann-Curvature/"/>
    
      <category term="Notation" scheme="http://yoursite.com/tags/Notation/"/>
    
  </entry>
  
  <entry>
    <title>Noether Theorem for Non-autonomous System</title>
    <link href="http://yoursite.com/2016/02/28/Noether-Theorem-for-Non-autonomous-System/"/>
    <id>http://yoursite.com/2016/02/28/Noether-Theorem-for-Non-autonomous-System/</id>
    <published>2016-02-28T10:11:44.000Z</published>
    <updated>2017-11-19T12:15:35.991Z</updated>
    
    <content type="html"><![CDATA[<p>鉴于大多数物理书混淆概念，把 $\text{Noether}$ 定理讲得天花乱坠还无比繁杂，这里给出真正正确清晰的定理表述，以正视听。</p><a id="more"></a><p>自治系统是下面非自治系统的一般情况，或参见[1].</p><p>以下约定记号: 下标为一的是非自治系统物理量, 无下标为自治系统的. 自治系统 $\text{Lagrangian}$ 含时, 将时间并入广义坐标, 引入参数 $\tau$, 使得真实运动 $x^{\mu}=\varphi(\tau)$ (约定 $x^{0}\equiv t$, $\mu=1,2,3$. 位形流形 $M_{1}=M\times\mathbb{R}$.</p><hr><p><strong>定义 1：</strong>位形流形 $M_1$ 上的拉氏量 $\mathcal{L}_1:TM\rightarrow\mathbb{R}$ 定义为<br>$$\mathcal{L}_1\left(t,\bm{q},\dfrac{\partial t}{\partial\tau},\dfrac{\partial\bm{q}}{\partial\tau}\right):=\mathcal{L}\left(t,\dfrac{\partial\bm{q}/\partial\tau}{\partial t/\partial\tau},\bm{q}\right)\dfrac{\rm{d}t}{\rm{d}\tau}.$$</p><hr><p><strong>定义 2：</strong>称光滑映射 $h$ 为容许映射, 若满足推前映射下拉式量不变, 即 $$\displaystyle\forall \bm{v}\in TM_{1}, \mathcal{L}_1(h_{*}\bm{v})=\mathcal{L}_1(\bm{v})$$.</p><hr><p><strong>定理 1（Noether）：</strong>若系统 $(M_1,\mathcal{L}_1)$ 容许单参微分同胚映射 $h^{s}:M_1\rightarrow M_1$, 则存在首次积分<br>$$\begin{equation}\displaystyle I_{1}=\dfrac{\partial\mathcal{L}_{1}}{\partial\left(\dfrac{\mathrm{d} x^{\mu}}{\mathrm{d}\tau}\right)}\cdot\dfrac{\partial h^{s}(x^{\mu})}{\partial s}.\end{equation}$$</p><hr><p><strong>证明：</strong>如前, 记 $x^{\mu}=\varphi(\tau)$ 为真实运动, 满足 E-L 方程. 依容许映射定义, 容许映射保持拉氏量不变, 故 $h^{s}x^{\mu}=:\Phi(\tau,s)$ 也满足 E-L 方程. 拉式量不变意为<br>$$0\equiv\dfrac{\partial\mathcal{L}_{1}}{\partial s}=\dfrac{\partial\mathcal{L}_{1}}{\partial x^{\mu}}\cdot\dfrac{\partial\Phi}{\partial s}+\dfrac{\partial\mathcal{L}_{1}}{\partial\left(\dfrac{\mathrm{d}x^{\mu}}{\mathrm{d}\tau}\right)}\cdot\dfrac{\partial}{\partial s}\left(\dfrac{\mathrm{d}\Phi}{\mathrm{d}\tau}\right).$$<br>改 $\tau$ 为参数后, E-L 方程成为<br>$$\dfrac{\mathrm{d}}{\mathrm{d}\tau}\dfrac{\partial\mathcal{L}_{1}}{\partial\left(\dfrac{\mathrm{d}x^{\mu}}{\mathrm{d}\tau}\right)}=\dfrac{\mathcal{L}_{1}}{\partial x^{\mu}}.$$</p><hr><p><strong>推论 1：</strong>首次积分可以明显写为<br>$$\begin{equation}\displaystyle I_{1}=-\mathcal{H}\dfrac{h^{s}(t)}{\partial s}+\dfrac{\partial\mathcal{L}}{\partial\dot{\bm{q}}}\cdot\dfrac{h^{s}(\bm{q})}{\partial s}.\end{equation}$$</p><hr><p><strong>证明：</strong>对前半项的 $x^{0}$ 分量, 有<br>$$\begin{align}<br>\dfrac{\partial\mathcal{L}_{1}}{\partial\left(\dfrac{\mathrm{d} x^{0}}{\mathrm{ d}\tau}\right)}  &amp;= \dfrac{\partial}{\partial\left(\dfrac{\mathrm{d}x^0}{\mathrm{d}\tau}\right)}\left(\mathcal{L}\left(t,\dfrac{\mathrm{d}\bm{q}/\mathrm{d}\tau}{\mathrm{d}t/\mathrm{d}\tau}\right)\dfrac{\mathrm{d}t}{\mathrm{d}\tau}\right)\nonumber\\<br>&amp;=\mathcal{L}+\left[\dfrac{\partial\mathcal{L}}{\partial\dot{\bm{q}}}\cdot\left(-\dfrac{\mathrm{d}\bm{q}/\mathrm{d}\tau}{(\mathrm{d}t/\mathrm{d}\tau)^{2}}\right)\right]\cdot\dfrac{\mathrm{d}t}{\mathrm{d}\tau}\nonumber\\<br>&amp;=\mathcal{L}-\dot{\bm{q}}\dfrac{\partial\mathcal{L}}{\partial\dot{\bm{q}}}=\mathcal{H}.\nonumber<br>\end{align}$$<br>其余同理, 留作练习.</p><hr><p>参考文献：</p><p>[1] Anold, 经典力学中的数学方法. 高教社.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;鉴于大多数物理书混淆概念，把 $\text{Noether}$ 定理讲得天花乱坠还无比繁杂，这里给出真正正确清晰的定理表述，以正视听。&lt;/p&gt;
    
    </summary>
    
      <category term="Mathematical Physics" scheme="http://yoursite.com/categories/Mathematical-Physics/"/>
    
    
      <category term="Conservation Current" scheme="http://yoursite.com/tags/Conservation-Current/"/>
    
      <category term="Noether Theorem" scheme="http://yoursite.com/tags/Noether-Theorem/"/>
    
      <category term="Symmetry" scheme="http://yoursite.com/tags/Symmetry/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2016/01/02/hello-world/"/>
    <id>http://yoursite.com/2016/01/02/hello-world/</id>
    <published>2016-01-02T12:23:31.000Z</published>
    <updated>2017-11-19T14:32:24.743Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p><h3 id="Syntax-Test"><a href="#Syntax-Test" class="headerlink" title="Syntax Test"></a>Syntax Test</h3><h4 id="Footnote"><a href="#Footnote" class="headerlink" title="Footnote"></a>Footnote</h4><p>I love you<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;">test</span><a href="#fnref:1" rev="footnote"> ↩</a></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
